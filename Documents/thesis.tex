\input{preamble/preamble.tex}
\graphicspath{{images/}}
\begin{document}

\includepdf[pages={-}]{preamble/begining.pdf}
\tableofcontents
\thispagestyle{empty}
\thesisstyle
\newpage 

\section[Wstęp]{Wstęp}
W dzisiejszych czasach techniki 3D są rozwijane na podstawie opracowań efektu stereoskopowego. Ogólnie, stereoskopia to technika służącą do tworzenia iluzji głębi obrazu, nie zapewnia ona prawdziwie trójwymiarowych widoków, ale zapewnia efekt trójwymiarowości. Każdemu oku obserwatora jest prezentowany inny widok, dlatego sceny wydają się mieć głębię.

Historia stereoskopii liczy już ponad 180 lat. Za odkrywcę uważa się Charlesa Wheatstone'a, który jako pierwszy zaobserwował zjawisko widzenia stereoskopowego, opisał go w artykule oraz skonstruował urządzenie potwierdzające słuszność jego teorii \cite{stereoscopehistory}. Wheatstone wyjaśnia, że jeśli obiekt znajduje się w małej odległości od oczu, to każde oko widzi przedmiot pod nieco innym kątem, z innej perspektywy. Zaś dla obiektów na dużej odległości nie ma to znaczenia \cite{wheatstone}. Wykorzystując te obserwacje naukowiec skonstruował urządzenie (stereoskop), które umożliwiło przedstawienie delikatnie różniących się obrazów prawemu i lewemu oku z osobna z powstającą w efekcie iluzją trójwymiarowości obrazu.

Do około 1930 r. stereoskop łączono z nowo rozwijającym się aparatem fotograficznym, takie połącznie zrobiło się niezwykle popularne szczególnie wśród ludzi epoki wiktoriańskiej. Od około 1950 do wczesnych lat 1970-tych, stereografia wzbudziła kolejne zainteresowanie wraz ze wzrostem użycia drukowanych obrazów anaglifowych i spolaryzowanego światła w kinematografii. Jednak zanim nie były wystarczająco rozwinięte środowiska obliczeniowe i dopóki nie opracowano odpowiednio dużej rozdzielczości wyświetlaczy, ciężko było mówić o dużym postępie grafiki komputerowej i o kroku w kierunku wirtualnej rzeczywistości. Kolejne duże zainteresowanie tym tematem powstało na początku lat 90, ponieważ zaczęto korzystać z wizualizacji stereoskopowych do celów badawczych i naukowych. Wtedy też wynaleziono pierwsze okulary migawkowe z soczewkami z ciekłych krystałów oraz projektory CRT pozwalające na znaczne ulepszenie jakości stworzonych obrazów stereoskopowych.

Technologie wyświetlania i wizualizacji od 1990-tych rozwijały się w każdym kierunku i dzisiaj można zauważyć na rynku zarówno większe i mniejsze ekrany z coraz większą rozdzielczością. Telewizja i okulary 3D stają się coraz bardziej powszechne dzięki najnowszym trendom w elektronice użytkowej i postępowi technologicznemu w dziedzinie technologii noszonych. Istnieje obecnie wiele opcji sprzętowych, które umożliwiają nam wizualizację informacji za pomocą stereoskopowej technologii 3D. 

Trzeci wymiar zapewnia nowy stopień swobody projektantom, a iluzja głębokości może być wykorzystana zarówno do wizualizacji statycznych, jak i do interaktywnego użytkownia. Możliwość wizualizacji danych w stereoskopowym środowisku 3D udostępnia potężną i wysoce urozmaiconą płaszczyznę do interaktywnego wyświetlania informacji w wielu aplikacjach. Staje się możliwe pozyskiwanie danych po zeskanowaniu rzeczywistych modeli 3D (na przykład w architekturze czy inżynierii). Znalazło to wykorzystanie również w fizyce dużych energii. 

ALICE (ang. A Large Ion Collider Experiment, czyli Wielki Eksperyment Zderzacza Jonów) jest jednym z największych na świecie eksperymentów fizycznych. Celem tego eksperymentu jest pomiar zderzeń ciężkich jonów przy najwyższych osiągalnych obecnie energiach przy użyciu detektora na Wielkim Zderzaczu Hadronów w międzynarodowym laboratorium fizyki cząstek CERN w Genewie. 

Tak na przykład graficzne symulacje zderzeń cząstek pozwalają na przedstawienie zachodzących zjawisk i procesów wewnątrz akceleratora, których bez komputera nie dałoby się poznać. Komputerowo generowane obrazy mogą przedstwiać efekt końcowy zderzenia w różnych okresach czasowych z różnymi wartościami parametrów fizycznych. Przy połączeniu fizyki dużych energii i grafiki komputerowej powstaje możliwość dogłębnego przestudiowania zjawisk zachodzących podczas rożnych eksperymentów w CERN. 

Wykorzystanie technologii stereoskopowych na studiach zwiększyłoby zainteresowanie studentów konkretnym może mniej ciekawym tematem. Jako praktyczne i interaktywne podejście do pokazania pewnych rzeczy dostarcza nowych, potężnych doświadczeń, z którymi wcześniej grupa docelowa najprawdopodobniej nie miała do czynienia. Doświadczenia takie zapewniają wyjątkowe i świeże chwile w przyswajaniu materiału, możliwość interaktywnego eksplorowania wiedzy mogłaby przyciągnąć uwagę studentów i wzbudzić w nich większe zainteresowanie. 

Wizją niniejszej pracy dyplomowej jest zwiększenie atrakcyjności tematu akceleratora w CERN poprzez udostępnienie stworzonych widoków stereoskopowych. Efekt głębi obrazu umożliwiłby studentom widzenie detektora ALICE i torów cząstek po zderzeniu w trzech wymiarach, przybliżając tym samym zagadnienie fizyki dużych energii. Wsparciem do spełnienia tej misji posłużyłoby odpowiednie oprogramowanie. 

Program by zaimplementowano z użyciem biblioteki OpenGL umożliwiającej tworzenie obrazów stereoskopowych stosunkowo niskim kosztem. W takim przypadku nie byłoby konieczne sprzętowe wsparcie stereografii. OpenGL jest biblioteką ogólnodostępną, z dużą liczbą samouczków do tworzenia oprogramowania graficznego o wysokim poziomie jakości. 

\subsection{Cele pracy}
Za cele niniejszej pracy dyplomowej przyjęto:
\begin{itemize}
\itemi analizę i prównanie dostępnych narzędzi graficznych;
\itemi stworzenie oprogramowania z wykorzystaniem biblioteki OpenGL, które:
	\begin{itemize}
	\itemii wyświetla realistyczny obraz detektora ALICE;
	\itemii zobrazowuje zderzenia ciężkich ionów w eksperymencie ALICE;
	\end{itemize}
\itemi osiągnięcie efektu głębi w stworzonych obrazach za pomocą dwóch technologii;
\itemi zapewnienie przenośności oprogramowania (działa zarówno pod systemem operacyjnym Linux jak i Windows).
\end{itemize}

\subsection{Słownik użytych terminów}
Ze względu na brak jednolitego tłumaczenia niektórych zasadniczych terminów z języka angielskiego na język polski, przedstawione jest nazewnictwo użyte w pracy dyplomowej. Tłumaczenia były zbierane głównie w oparciu o różne słowniki oraz literaturę specjalistyczną.
\begin{itemize}
\itemi Renderować (-ruje) --- ang. rendering, tworzyć realistyczne obrazy komputerowe \cite{pwn}.
\itemi Shader (-ra, -rze; -ry, -rów) --- moduł cieniujący, nieduży program w procesorze graficznym \cite{slownik}.
\itemi Antyaliasing (-ngu, -ngiem)  --- ang. anti-aliasing, zespół technik służących usuwaniu zakłóceń powstających przy renderowaniu obrazu \cite{wprowadzeniedografiki}.
\itemi Asemblacja --- ang. assembling, montaż, zebranie, zgromadzenie prymitywu \cite{slownik}.
\itemi Teselacja --- ang. tessellation, dzielenie wygenerowanych podczas tworzenia obrazu 3D wielokątów na mniejsze \cite{slownik}.
\itemi Rasteryzacja --- ang. rasterisation, działanie polegające na jak najwierniejszym przedstawieniu płaskiej figury geometrycznej na urządzeniu rastrowym \cite{slownik}.
\itemi Track (-a, -ów) --- ślad, ścieżka poruszania się cząstki w detektorze.
\itemi Uniform (rmu, -rmie; -rmów) --- zmienna globalna do przekazywania danych z aplikacji do procesora i z procesora do modułów cieniujących \cite{slownik}.
\end{itemize}

\newpage
\section[Część analityczna]{Część analityczna}
\subsection{Stereoskopia}
Istnieje kilka różnych technik osiągnięcia efektu stereoskopowego. Każda technika ma własny materiał stereoskopowy, inaczej stereogram, oraz sposób przeglądania dostosowany do konkretnego celu. W dalszej części niniejszej pracy są przedstawione krótkie opisy kilku technik streoskopowych cieszących się dużą rozpoznawalnością. Niektóre z nich są odpowiednie tylko dla jednego widza, podczas gdy istnieją techniki sprawdzające się dla większej grupy odbiorców.

\subsubsection{Stereogramy typu side by side}
Najprostszą metodą doświadczenia widzenia stereoskopowego jest umieszczenie dwóch oddzielnych obrazów obok siebie (ang. side by side). Perspektywa jednego z obrazów reprezentuje lewe oko, odpowiednio druga perspektywa jest dla oka prawego. Obrazy te można zazwyczaj oglądać bez użycia dodatkowych narzędzi. Jedyna różnica polega na sposobie ich oglądania. Jeśli obraz dla lewego oka znajduje się po lewej stronie, to stosuje się technikę patrzenia równolegległego, w innym przypadku obrazy mają być oglądane z użyciem techniki skrośnej. Zastosowanie niewłaściwej metody spowoduje odwrócenie efektu stereoskopowego. Może to ujawnić się w postaci zniekształconych odległości pomiędzy obiektami na obrazie lub oddalenia się dwóch obrazów.

Warto zauważyć, że oglądanie stereogramów tworzy duże obciążenie dla oczu. W celu zmniejszenia negatywnych skutków używa się stereoskopu bądź specjalnego ekranu 3D w zestawie z okularami. To w znacznej mierze redukuje przemęczenie oczu i pozwala przez dłuższy czas przyglądać się efektom widzenia stereoskopowego.
\begin{figure}[H]
		\centering
 		\includegraphics[width=10cm]{sbs.jpg}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Stereogram typu side by side\cite{sidebyside}.}
 		\label{rys1}
\end{figure}

\subsubsection{Stereogramy typu single-image} 
Stereogramy typu single-image, inaczej nazywane autostereogramy, to zbiór punktów, które wydają się tworzyć trójwymiarową scenę, gdy są skrośnie oglądane z bliska. Do osiągnięcia efektu stereoskopowego za pośrednictwem autostereogramów nie są potrzebne żadne urządzenia, aczkolwiek okulary pryzmatyczne ułatwiają przegladanie poprzeczne, a także nad- i podgląd.

Przy użyciu poprawnej techniki oglądania specjalnie stworzonego obrazu powstaje wrażenie trójwymiarowości. Czasem obrazy mogą wyglądać jak zniekształcone lub losowo umieszczone kolorowe plamy. Taki wygląd w rzeczywistości nie jest przypadkowy, ponieważ poprzez użycie powtarzalnego wzoru są ukrywane obiekty trójwymiarowe \cite{stereoscopythesis}.

\subsubsection{Stereoskopia pasywna} 
Do tworzenia iluzji obrazów trójwymiarowych w tej technice są wykorzystywane specjalnie spolaryzowane okulary, które ograniczają światło docierające do każdego oka. W celu przedstawienia widoku stereoskopowego dwa obrazy są wyświetlane na tym samym ekranie z użyciem różnych filtrów polaryzacji. Istenieją 2 rodzaje filtrów: liniowe oraz kołowe.

W przypadku liniowej polaryzacji pole elektryczne światła jest skierowane pionowo bądź poziomo. Odbiorca zakłada odpowiednio dopasowane okulary, w których każda soczewka przepuszcza światło o innym kierunku polaryzacji. W ten sposób oczy obserwują różniące się obrazy. Minimalne pochylenie okularów może spowodować niezgodność polaryzacji światła a soczewek. Nieporządany efekt można wyeliminować korzystając z filtrów kołowych prawo- lub lewoskrętnych. Ruch ten pozostaje niezmienny, nawet jeśli okulary są dowolnie pochylone.

Niewątpliwą zaletą stereoskopii spolaryzowanej jest to, że wiele osób może oglądać obrazy stereoskopowe w tym samym czasie \cite{russianpage}. Wadą tego rozwiązania jest koszt specjalnego ekranu do wyświetlania obrazów. Taki ekran musi posiadać srebrną warstwę, dzięki czemu nie zniekształca się polaryzacja obrazów przy jednoczesnym wyświetlaniu.
\begin{figure}[H]
		\centering
 		\includegraphics[width=8cm]{circular.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Polaryzacja kołowa prawoskrętna \cite{polarization}.}
 		\label{rys3}
\end{figure}

\subsubsection{Stereoskopia aktywna} 
Z użyciem techniki stereoskopii aktywnej możliwość oglądania obrazów stereoskopowych pojawia się dzięki ciekłokrystalicznym okularom migawkowym. Szkło, które tworzy soczewki okularów, zawiera ciekłe kryształy oraz filtr polaryzacyjny. Stosując napięcie można zmienić kolor soczewki na czarny, co bezpośrednio wiąże się z blokowaniem widoku dla jednego oka. Dzięki przykrywaniu widoku z dużą częstotliwością, każdemu oku są przedstawiane obrazy z różną perspektywą \cite{active3d}. Częstotliwość migotania okularów jest zsynchronizowana z aktualnym źródłem wyświetlania, żeby stale dostarczać prawidłowej perspektywy dla oczu. Jeśli prędkość aktualizacji soczewek w okularach nie jest wystarczająco wysoka, to może nastąpić zauważalne zniekształcenie obrazu końcowego. Przykładem takiego zniekształcenia może posłużyć część obrazu ze złą perspektywą, przeznaczona innemu oku.
\begin{figure}[H]
		\centering
 		\includegraphics[width=6cm]{telewizor.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Stereoskopia aktywna \cite{active3d}.}
 		\label{rys2}
\end{figure} 

\subsubsection{Techniki anaglifowe}
Do generowania stereogramu anaglifowego potrzebne są 2 obrazy z delikatnie różniącą się perspektywą dla każdego oka. Każdy obraz jest wykonany w podstawowych kolorach. Najbardziej popularną kombinacją jest obraz w kolorze czerwonym dla lewego oka i obraz wykonany w przy połączeniu zielonego i niebieskiego (otrzymany kolor nazywa się cyan) odpowiednio dla prawego oka. Obrazy umieszcza się jeden nad drugim. Efekt stereoskopowego widoku jest osiągany przy użyciu okularów z poprawnymi kolorowymi filtrami (podobnie jak obrazy: czerwony dla lewego oka, cyan dla prawego). Niestety przy użyciu tej techniki nigdy nie zostanie stworzony pełnokolorowy obraz, gdyż barwy widziane przez każde oko są ograniczane \cite{anaglif}. 

Istenieją także filtry o innych kolorach, zachowując główną zasadę anaglifów: przedstawienie innych obrazów każdemu oku poprzez ograniczenie pewnych barw.

\begin{figure}[H]
		\centering
 		\includegraphics[width=7cm]{anaglif.jpg}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Anaglif \cite{anaglif}.}
 		\label{rys4}
\end{figure}

\subsubsection{Wizualizacja wielopasmowa}
Nowa technika wyświetlania obrazów stereoskopowych, znana również pod tytułem INFITEC (ang. Interference filter technique, technika filtrowania interferencyjnego). Główna idea tej techniki jest podobna do technik anaglifowych: pokazanie dwóch obrazów z różniącą się perspektywą, które są wykonane w różnych kolorach. Jednak do osiągnięcia efektu stereoskopowego za pośrednictwem wizualizacji wielopasmowych jest wykorzystany taki fakt, że widmo światła widzialnego można podzielić na fale o różnej długości używając do tego filtrów optycznych \cite{infitec}.

Światło widzialne jest podzielone na 6 części, po 2 na każdy z podstawowych kolorów: czerwony, zielony oraz niebieski. Poszczególne części przeznaczone są dla jednego oka. W ten sposób lekko różniące się barwy docierają do każdego oka z osobna. Wykorzystując niewystarczającą wrażliwośc mózgu na tak niedużą różnicę, technika wizualizacji welopasmowej stwarza obrazy pełnokolorowe.  

\subsubsection{Porównanie technik stereoskopowych}
\begin{table}[H]
\caption{Porównanie technik stereoskopowych.}
\centering
\footnotesize
\label{tab11}
\begin{tabular}{!{\color{sapphire}\vrule width 1pt}m{0.146\textwidth}!{\color{black}\vrule width 1pt}m{0.146\textwidth}!{\color{black}\vrule width 1pt}m{0.146\textwidth}!{\color{black}\vrule width 1pt}m{0.146\textwidth}!{\color{black}\vrule width 1pt}m{0.146\textwidth}!{\color{black}\vrule width 1pt}m{0.146\textwidth}!{\color{sapphire}\vrule width 1pt}}
	\arrayrulecolor{sapphire}\hline
	\Centering\bfseries Nazwa technologii &
	\Centering\bfseries Kolory &
	\Centering\bfseries Rozdzielczość &
	\Centering\bfseries Specjalny monitor &
	\Centering\bfseries Liczba widzów &
	\Centering\bfseries Koszt \\
	\hline
	\arrayrulecolor{black}
	Side by side & Obraz pełnokolorowy & Wysoka & Niekonieczny & Ograniczona & Średni \\ 
	\hline
	Single image & Całkowity brak kolorów & Średnia & Nie & Ograniczona & Niski \\ 
	\hline
	Stereoskopia pasywna & Obraz pełnokolorowy & Wysoka & Tak & Duża & Średni \\ 
	\hline
	Stereoskopia aktywna & Obraz pełnokolorowy & Wysoka & Tak & Ograniczona & Wysoki \\ 
	\hline
	Anaglify & Całkowity brak kolorów & Średnia & Nie & Duża & Bardzo niski \\ 
	\hline
	Wizualizacje wielopasmowe & Obraz pełnokolorowy & Wysoka & Nie & Ograniczona & Bardzo wysoki \\ 
	\arrayrulecolor{sapphire}\hline
\end{tabular}
\end{table}

Po porównaniu opisanych technik stereoskopowych zostały wybrane 2 do głębszego przestudiowania: technika stereoskopii pasywnej oraz technika stereoskopii aktywnej. Zostaną one użyte w oprogramowaniu końcowym.
\newpage
\subsection{Nvidia 3D Vision --- implementacja stereoskopii aktywnej}
Nvidia --- światowy lider w dziedzinie technologii obliczeń graficznych i wynalazca GPU (ang. Graphics Processing Unit, wysokowydajny procesor graficzny, potocznie nazywany kartą graficzną), który potrafi tworzyć realistyczne interaktywne obrazy na stacjach roboczych, komputerach osobistych, konsolach do gier i urządzeniach mobilnych \cite{NvidiaInfo}. 

Jedną z najbardziej znanych marek procesorów graficznych jest GeForce (ang. Geometry Force, siła geometrii), zaprojektowany przez Nvidię, początkowo przeznaczony wyłącznie dla graczy. Obecnie architektura GeForce jest rozwiajana w kierunku procesora graficznego ogólnego przeznaczenia (ang. GPGPU, general-purpose graphics processor unit). Ma to rozszerzyć funkcjonalność GPU poza tradycyjną rasteryzację grafiki 3D, przekształcając go w bardzo wydajne urządzenie komputerowe, które potrafi wykonywać kod dowolnego oprogramowania w taki sam sposób, jak robi to zwykły procesor \cite{GeForce}. 

Dla bezproblemowego korzystania z technologii wystawaniono następujące wymagania systemowe: 
\begin{itemize}
\itemi system operacyjny Microsoft Windows Vista 32(64) bity lub Windows 7 32(64) bity;
\itemi procesor  Intel Core2 Duo lub AMD Athlon X2 (ewentualnie nowsze wersje);
\itemi co najmniej 1GB pamięci RAM, aczkolnwiek zaleca się 2GB;
\itemi minimum 100 MB wolnej pamięci na dysku;
\end{itemize}

\subsubsection{Szegółowy opis zestawu 3D Vision}
3D Vision (wcześniej GeForce 3D Vision) to zestaw dla widzenia stereoskopowego również opracowany przez firmę Nvidia. Składa się zestaw z okularów migawkowych i oprogramowania sterownika. 3D Vision jest skierowany głównie do graczy i twórców gier komputerowych, gdzyż gry 3D są dodatkową płaszczyzną do rozwoju. Sterownik Nvidia 3D Vision może przetwarzać dowolną grę na wyświetlaczach o wystarczająco dużej szybkości odświeżania \cite{NvidiaInfo}. 

\begin{figure}[H]
		\centering
 		\includegraphics[width=9cm]{3dVision.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{3D Vision wraz z diagramem poprawnego ustawienia \cite{3dVisionPic}.}
 		\label{rys4}
\end{figure}
Podstawowy zestaw 3D Vision według oficjalnej strony internetowej \cite{3dVisionInfo} składa się z: 
\begin{itemize}
\itemi aktywnych okularów migawkowych zapewniających dwukrotnie lepszą rozdzielczość dla każdego oka i szerszy kąt widzenia w porównaniu do okularów pasywnych;
\itemi emitera podczerwieni wysokiej mocy przesyłającego dane bezpośrednio do aktywnych okularów migawkowych;
\itemi maksymalnej elastyczności wyświetlacza zaprojektowanego dla wyświetlaczy z częstotliwością odświżania co najmniej 120 Hz, pozwalających uzyskać krystalicznie czyste, pozbawione migotania stereoskopowe obrazy 3D;
\itemi bezpłatnej przeglądarki 3D Vision pozwalającej użytkownikom robić zrzuty ekranu w grze i wyświetlać je w 3D;
\itemi zaawansowanego oprogramowania Nvidia automatycznie konwertującego ponad 300 gier do wyświetlenia w technologii aktywnej stereoskopii. Jest to również jedyne rozwiązanie do gier 3D w pełni obsługujujące technologie Nvidia SLI, Nvidia PhysX i Microsoft DirectX.
\end{itemize}

\subsubsection{Sposób działania sterownika}
Wewnątrz sterownika każda scena 3D jest tworzona dwukrotnie - raz dla lewego oka i raz dla prawego. Sterownik jest w stanie zmodyfikować typowe moduły cienijące gier czasie rzeczywistym, żeby móć generować poprawne obrazy w trakcie wykonywania programu. Opcje dostępne użytkownikom głównie pozwalają na dostosowanie indywidualnych ustawień, takie jak odległość między oczami czy wartość głębi obrazu. Natomiast twórcom oprogramowania umożliwiono bezpośrednie sterowanie aspektami stereo w badaniach. 

Niedużym nakładem prac programiści mogą przejąć kontrolę nad wyświetlaniem 3D zmieniając subtelne elementy sterujące odtwarzaniem lub dodając efekty specjalne poza ekranem \cite{3dVisionInfo}. 

W celu umożliwienia jak największej liczbie programistów stworzenie obrazów stereoskopowych dla wyświetlenia aktywnego jest rozwiajana technologia Nvidia 3D Vision Automatic. W tej chwili ta technika nie jest jeszcze dostępna dla OpenGL, lecz została opublikowana działająca wersja dla OpenGL ES pod systemem Android 3.2 (lub nowsza wersja) dla urządzeń z procesorem Tegra 3 (lub nowszym).

\newpage
\subsection{Biblioteka OpenGL}
OpenGL (ang. Open Graphics Library, otwarta biblioteka graficzna) jest potężnym systemem graficznym stanowiącym niejako pomost między programistą a sprzętem komputerowym. Procedury OpenGL umożliwiają renderowanie obiektów o rożnych poziomach skomplikowania zaczynając od prostego punktu geometrycznego, linii lub wypełnionego wielokąta do utworzenia najbardziej złożonej, zakrzywionej powierzchni, oświetlonej i odwzorowanej teksturą. OpenGL pozwala programistom na dostęp do prymitywów geometrycznych i obrazowych, przekształcenia modelu, oświetlenia i teksturowania, antyaliasingu i wielu innych funkcji. 

OpenGL obsługuje aplikacje wizualizacji z obrazami 2D traktowanymi jako typy prymitywów, którymi można manipulować podobnie jak obiektami geometrycznymi 3D. Mimo że specyfikacja biblioteki definiuje konkretny potok przetwarzania graficznego, dostawcy platformy mają swobodę dostosowywania konkretnej implementacji OpenGL, żeby osiągnąć sprecyzowane cele w zakresie kosztów i wydajności. Pojedyńcze wywołania mogą być wykonywane na dedykowanym sprzęcie, uruchamiane jako procedury programowe w standardowym systemie CPU (ang. Central Processing Unit, procesor) lub implementowane jako kombinacja zarówno specjalnych procedur sprzętowych, jak i programowych. Taka elastyczność implementacji przekłada się na przyspieszenie tworzenia realistycznych obrazów. Dodatkowo, biblioteka OpenGL jest powszechnie dostępna na wszystkich jednostkach od komputerów o niskich kosztach, po wysokiej klasy stacjach roboczych i superkomputerach \cite{openglofficial}.
\subsubsection{Potok przetwarzania}
\begin{figure}[H]
		\centering
 		\includegraphics[width=15.5cm]{OpenGL.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Schemat potoku przetwarzania.}
 		\label{rys7}
\end{figure}
W OpenGL wszystko jest przedstawione w przestrzeni trójwymiarowej, ale na ekranie obraz widzimy jako listę pikseli 2D, w związku z tym duża część pracy OpenGL polega na zmianie współrzędnych 3D na piksele 2D, które by pasowały do ekranu. Cały proces transformacji jest zarządzany poprzez potok graficzny OpenGL. Taki ciąg jest podzielony na poszczególne kroki, gdzie na wejściu są wymagane dane wyjściowe poprzedniego kroku. Każda z operacji jest dobrze sprecyzowana, gdyż mają one konkretną funkcję. Większość współczesnych kart graficznych posiada setki, czasmi tysiące, małych jąder procesowych do szybkiego przetwarzania danych wejściowych.

\subsubsection{Moduły cieniujące}
Poprzez zgrupowanie poszczególnych kroków w potoku przetwarzania OpenGL powstaje schemat uproszczony. Został on przedstawiony na rysunku \ref{rys8}.
\begin{figure}[H]
		\centering
 		\includegraphics[width=11cm]{pipeline.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Skrócony schemat potoku przetwarzania \cite{opengltutorial}.}
 		\label{rys8}
\end{figure}
 
 Dla każdego kroku przetwarzania, w celu przyśpieszenia obliczeń, są uruchamiane nieduże programy w procesorze graficznym --- moduły cieniujące.
 
Te programy są pisane w języku w dużym stopniu podobnym do C --- GLSL (ang. OpenGL Shading Language, język cieniowania OpenGL). GLSL jest dostosowany do wykorzystania w grafice, ponieważ zawiera przydatne funkcje skierowane na manipulacje wektorami i macierzami. 

Moduły cieniujące zawsze zaczynają się od deklaracji wersji, następnie deklarowane są listy zmiennych wejściowych i wyjściowych, uniformy oraz ich główne funkcje. Uniform --- sposób przekazywania danych z aplikacji do procesora i z procesora do modułów cieniujących. Uniformy to zmienne globalne \cite{slownik}. Prowadzi to do tego, że uniform jest unikalny dla każdego obiektu modułu cieniującego i jest osiągalny z dowolnego modułu w każdej chwili jego działania. Ponadto, bez względu na ustawioną wartość uniformy jej nie zmieniają, dopóki nie zostaną zresetowane lub zaktualizowane.
\begin{itemize} 
\itemi Moduł cieniujący wierzchołków (ang. vertex shader). Jest programem, który realizuje pierwszy i zarazem fundamentalny etap potoku ---przetwarzanie danych związanych z wierzchołkami, w tym różnych atrybutów przekazanych wraz z położeniem wierzchołka \cite{slownik}.
\itemi Montaż prymitywu (ang. primitive assembly). Jest to krok, który za dane wejściowe przyjmuje wszystkie wierzchołki (lub jeden, jeśli wybrana jest flaga GL\_POINTS ) z modułu cieniującego wierzchołków. Na tym etapie przetwarzania są kształtowane prymitywy --- wszystkie wierzchołki są grupowane do zadanego kształtu \cite{opengltutorial}.
\itemi Moduł cieniujący geometrii (ang. geometry shader). Program przyjmuje jako argumenty wejściowe kolekcję wierzchołków, które tworzą zadany kształt. W tym module również istenieje możliwość generowania innych kształtów poprzez emitowanie nowych wierzchołków tworząc nowe (lub inne) prymitywy \cite{slownik}. 
\itemi Rasteryzacja. W tym kroku uzyskane wcześniej prymitywy są mapowane na odpowiadające im piksele na ekranie. W wyniku powstają fragmenty do przetwarzania w kolejnym kroku, lecz zanim zostanie uruchomiony moduł cieniujący fragmentów, jest wykonywane przycinanie. Przyciannie pozwala odrzucić wszystkie fragmenty, które są poza zasięgiem obserwatora. Ów krok pozwala znacznie zwiększyć wydajność całego potoku. Fragment w OpenGL zawiera wszystkie niezbędne dane do wyświetlenia pojedyńczego piksela \cite{slownik}.
\itemi Moduł cieniujący fragmentów. Głównym celem tego programu jest obliczenie końcowego koloru piksela i jest to zazwyczaj etap, na którym występują wszystkie zaawansowane efekty OpenGL. Z reguły moduł posiada wszystkie dane o scenie 3D (takie jak światła, cienie, kolor światła itp.), które są niezbędne dla określenia ostatecznej wartości koloru piksela \cite{slownik}.
\end{itemize}

\newpage
\subsection{Formaty plików 3D}
W celu ponownego wykorzystania skonstruowanych modeli 3D i przesłania ich na różne platformy, tworzone są pliki graficzne. Jednak powstało wiele różnych formatów plików dla różnych aplikacji. Obecnie w grafice komputerowej istnieje ponad 70 odmiennych formatów \cite{formatslist}. Są one wykorzystywane w różnych dziedzinach zaczynając od druku 3D, gier komputerowych, aż po medycynę i nauki przyrodnicze. 

Podstawowym celem formatu pliku 3D jest przechowywanie informacji o modelu w postaci tekstu lub danych binarnych. W szczególności musi on zawierać dane o geometrii modelu, jego wyglądzie, scenie oraz animacjach. Geometria modelu opisuje jego dokładny kształt, do wyglądu zalicza się kolory, tekstury, typy wykorzystanych materiałów. Dane o scenie opisują między innymi położenie światła, kamery i obiektów pereferyjnych. 

U podstaw każdego opragramowania graficznego znajduje się biblioteka niskiego poziomu, taka jak OpenGL czy Direct3D. Biblioteki niskiego poziomu faktycznie rysują modele 3D na ekranie. Modele mogą być również przechowywane i przesyłane jako pliki graficzne. Narzędzia autorskie wspierają modelowanie, zapewniają użytkownikom wygodne metody tworzenia, przeglądania, modyfikowania i zapisywania stworzonych modeli. Często opragamowanie autorskie zawiera w sobie również przeglądarkę 3D --- narzędzie graficzne, które odczytuje, analizuje i transformuje pliki 3D do wewnętrznych formatów, a następnie wyświetla użytkownikowi. Przeglądarki grafiki 3D, narzędzia do tworzenia i transformatory formatów mogą uzyskiwać bezpośredni dostęp do plików 3D lub przechodzić przez funkcje bibliotek narzędzi programowania \cite{formatsinfo}. Opisane relacje są zilustrowane na rysunku \ref{rys5}.
\begin{figure}[H]
		\centering
 		\includegraphics[width=7.0cm]{relacje.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Relacje w narzędziach 3D.}
 		\label{rys5}
\end{figure}

W dalszej części tej pracy dyplomowej są przedstawione krótkie opisy najczęściej używanych formatów plików 3D. Na koniec tego podrozdziału formaty plików zostaną porównane pod względem rozmaitości informacji, które one są w stanie przechowywać. Ma to na celu wybranie formatu, który będzie wykorzystany w opracowywanym oprogramowaniu.

\subsubsection{Collada}
Collada to format pliku 3D z rozszerzeniem .DAE, powszechnie jest używany do modelowania w grach komputerowych i filmografii. W całości opiera się na strukturalizowaną reprezentację w XML. Przechowuje wszystkie dane dotyczące modelu 3D, jeden z niewielu formatów wspierających kinematykę i informacje o cieniowaniu \cite{colladaInfo}.

Struktura formatu zaczyna się w korzeniu zwanym COLLADA, który zawiera elementy mianowane bibliotekami i sceną. Element scena mieści w sobie odnośnik do rzeczywistego początku hierarchii sceny. Natomiast każdy element "biblioteka"\ składa się ze specjalnych zestawów danych dokładnie opisujących model: informacje o siatce (library\_geometries), obrazie (library\_images) itd. Taki podział jest bardzo wygodny pod względem odwoływania się do konkretnych sekcji.  

\subsubsection{STL}
STL (ang. STereoLithography, stereolitografia) jest jednym z najważniejszych formatów plików w dziedzinie druku 3D, tworzenia prototypów oraz komputerowo wspieranej produkcji. Rozszerzenie odpowiadające temu formatowi to .STL. Są dostępne oba rodzaje reprezentacji: tekstowy i binarny, przy czym binarny jest bardziej wykorzystywany ze względu na porównywalnie mały rozmiar plików. 

W STL są pominięte takie dane jak wygląd, scena czy animacje. Jedyne ważne informacje w tym formacie to geometria obiektów, która jest zapisywana w postaci przybliżonej siatki trójkątów. Dla każdego trójkąta są przechowywane 2 rodzaje danych:
\begin{itemize}
\itemi współrzędne wierzchołków;
\itemi współrzędne normalnej, przy tym wektor powinien wskazywać na zewnątrz w odniesieniu do modelu 3D.
\end{itemize}
\begin{figure}[H]
		\centering
 		\includegraphics[width=5.0cm]{vertices-and-normal.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Wierzchołki i normalna \cite{stlinfo}.}
 		\label{rys6}
\end{figure}
 Dotychczas jest to najprostszy i najbardziej ścisły format przechowywania informacji o modelu 3D. Wadą STL możnaby nazwać brak przechowywania danych o kolorach, ze względu na dynamiczny rozwój technologii druku pełnokolorowego \cite{stlinfo}.

\subsubsection{OBJ}
OBJ jest jednym z najbardziej znanych i wykorzystywanych formatów plików 3D, obecnie nabiera szczególnej wagi w druku 3D. Ten format przechowuje informacje o modelu w postaci tekstu ASCII (rozszerzenie .OBJ) bądź pliku binarnego (rozszerzenie .MOD) \cite{objinfo}.
Format binarny jest zastrzeżony i nieudokumentowany, dlatego poniżej zostanie opisana tylko postać tekstowa formatu. 

OBJ wspiera dane o prostych, wielokątach, krzywych oraz powierzniach o różnych kształtach. Proste i wielokąty są opisywane poprzez wierzchołki, z których się składają. W przypadku krzywych oraz płaszczyzn kluczowymi są punkty kontrolne i inne niezbędne do opisu informacje, w zależności od typu krzywej. Ten format pliku 3D przybliża bądź oblicza doskładną siatkę bez drastycznego zwiększania rozmiaru pliku. Staje się to możliwe dzięki wykorzystaniu krzywych Beziera oraz metody NURBS (ang. Non-Uniform Rational Bezier Spline). 

Format OBJ umożliwia również przechowywanie informacji o kolorach oraz teksturach modelu w towarzyszącym formacie MTL (ang. Material Template Library, Biblioteka Szablonów Materiałów). Plik .OBJ, po sparowaniu z odpowiednim plikiem .MTL, tworzy realistyczny wielokolorowy teksturowany model. Plik MTL jest zapisywany w postaci tekstu ASCII definiującego właściwości materiałów, odbijania światła itd. Dodatkowo MTL wspiera mapy tekstur, w tym przypadku każdy wierzchołek siatki modelu 3D jest przyporządkowywany do dwuwymiarowego obrazu \cite{mtlInfo}. 

\subsubsection{3DS}
3DS --- to binarny format pliku początkowo wykorzystywany tylko w programie Autodesk 3D Studio. Plik binarny jest oparty o hierarchiczną strukturę "klocków"\ (ang. chunks), w której każdy fragment danych jest umieszczony w bloku z odpowiednim identyfikatorem. Pozwala to analizatorowi składni pominąć fragmenty, które nie są rozpoznawalne, oraz zapewnia możliwość rozszerzenia formatu \cite{3dsformat}. 

3DS przechowuje tylko podstawowe informacje o geomertii, wyglądzie (kolory, tekstury i materiały), scenie oraz animacji. Gromadząc dane o scenie zapisuje położenie kamer i świateł, z wyjątkiem informacji o kierunkowych źródłach światła. Do przybliżenia kształtu obiektu w formacie 3DS używa się siatki trójkątów, ale liczba wierzchołków i wielokątów nie może przekraczać 65536 ($2^{16}$). 

Ten format jest obsługiwany praktycznie przez wszystkie pakiety oprogramowania 3D. Ze względu na to, że jednak 3DS zbiera tylko podstawowe informacje o modelu 3D, może on być uzupełniony o format MAX (obecnie zastąpiony formatem PRJ), który zawiera dodatkowe informacje specyficzne dla Autodesk 3DS Max, aby umożliwić całkowite zapisanie i załadowanie sceny.

\subsubsection{X3D}
Początkowo X3D nazywał się VRML (rozszerzenie pliku .WRL), z ang. Virtual Reality Modeling Language, czyli Język Modelowania Rzeczywistości Wirtualnej. Format został opracowany na potrzeby WWW (ang. World Wide Web), z czasem został zastąpiony przez X3D. Jest oparty o składnię XML.

X3D wykorzystuje siatkę wielokątów do zakodowania kształtu modelu, umożliwia przechowywanie wyglądu i danych, które wiążą się z tym parametrem. Na przeciągu ostatnich lat rozwoju tego formatu zostały dodane: kodowanie NURBS powierzchni geometrii, a także możliwość gromadzenia danych o scenie i animacjach.

\subsubsection{Porównanie formatów plików}
Tabela \ref{tab1} przedstawia porównanie opisanych formatów plików 3D pod względem różnorodności przechowywanych danych. 
\begin{savenotes}
\begin{table}[H]
\caption{Macierz funkcjonalności najpopularniejszych formatów plików 3D}
\centering
\footnotesize
\label{tab1}
  \begin{tabular}{!{\color{sapphire}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{black}\vrule width 1pt}c!{\color{sapphire}\vrule width 1pt}}
	\arrayrulecolor{sapphire}\hline
    \multirow{2}{*}{\bfseries Format} &
      \multicolumn{3}{c!{\color{black}\vrule width 1pt}}{\bfseries Geometria} &
      \multicolumn{3}{c!{\color{black}\vrule width 1pt}}{\bfseries Wygląd} &
      \multicolumn{3}{c!{\color{black}\vrule width 1pt}}{\bfseries Scena} &
     \multirow{2}{*}{\bfseries Animacje}\\
     \cline{2-10}
    & Przybl.\footnote{Przybliżona siatka}&Dokł.\footnote{Dokładna siatka}&CSG\footnote{CSG (ang. constructive solid geometry, strukturalna geometria bryły) – technika definiowania nowych brył poprzez łączenie innych brył}& Kolor& Materiał&Tekstura&Kamera&Światło&Pozycje& \\
    \hline
    Collada & X & X &  & X & X & X & X & X & X & X\\   
    \arrayrulecolor{black}
	\hline
    STL & X &  &  &  &  &  &  &  &  & \\
    \hline
    OBJ & X & X &  & X & X & X &  &  &  & \\
    \hline
    3DS & X &  &  & X & X & X & X & X & X & \\ 
    \hline
    X3D & X & X & X & X & X & X & X & X & X & X\\     
   \arrayrulecolor{sapphire}\hline
  \end{tabular}
\end{table}
\end{savenotes}
Z tabeli wynika, że najwięcej informacji przechowują formaty Collada i X3D. W niniejszej pracy dyplomowej zostanie wykorzystany format Collada, ponieważ pliki z rozszerzeniem .dae są łatwe w odczytaniu dzięki swojej strukturze. Do parsowania plików w wybranym formacie dostępne są również opracowane i przetestowane narzędzia. 

\newpage
\subsection{Biblioteki do czytania i pisania modeli 3D}
Niestety, graficzne biblioteki niskiego poziomu takie jak OpenGL czy DirectX nie zapewnią żadnego mechanizmu do ładowania, zapisywania lub manipulowania modelami 3D. Dlatego, powstała potrzeba stworzenia nowych bibliotek ułatwiających te czynności. 
\subsubsection{Assimp}
Assimp (ang. Open Asset Import Library, Biblioteka Importowania Zasobów) to przenośna biblioteka do importowania różnych dobrze znanych formatów modeli 3D w jednolity sposób. Najnowsza wersja potrafi nie tylko czytać, ale również i zapisywać pliki 3D i dlatego nadaje się jako transformator modeli 3D do ogólnego zastosowania. Assimp jest napisana w języku C ++, istnieje również API (ang. Application Programming Interface, Interfejs Programistyczny Aplikacji) w języku C, a także jest powiązana z innymi językami programowania, w tym C \#, .net, Python i D.

Assimp ładuje wszystkie formaty modeli wejściowych do jednej prostej struktury danych w celu dalszego przetwarzania. Podstawowy zestaw funkcji jest rozszerzany przez różne narzędzia do przetwarzania końcowego. Przykładem rozszerzenia mogą posłużyć często potrzebne operacje, takie jak obliczanie wektorów normalnych i stycznych \cite{assimp}.
\subsubsection{Lib3ds}
Lib3ds to darmowa alternatywa dla pakietu 3DS File Toolkit firmy Autodesk do zarządzania plikami 3DS. Ta biblioteka jest w całości napisana w języku C, wspierana przez takie platformy jak GNU, UNIX, Mac OS X, Microsoft Visual C++ 8.0. Jest łatwo integrowalna z biblioteką OpenGL \cite{lib3dsofficial}. 

Celem lib3ds jest uproszczenie tworzenia filtrów do czytania i pisania plików formatu 3DS. Ta biblioteka jest obsługiwana na różnych rodzajach procesorów (big oraz little endian), wspiera moduły wektorowe, kwaterniony, macierze, proste struktury danych, które można łatwo modyfikować, ocenę wszystkich danych animacji. Istnieje możliwość załadowania większości fragmentów 3DS, sekcje: materiały, kamera, światło, siatka i klucze \cite{lib3dsdirectory}.

\newpage
\subsection{ALICE}
Detektor ALICE został zaprojektowany tak, aby w jak najbardziej kompletny sposób badać cząstki powstałe w kolizjach, które mają miejsce w środku akceleratora. Do zrealizowania założeń, używa się wielu warstw różnych detektorów, z których każda dostarcza innych informacji o przebiegu zderzenia. Dane są gromadzone w sposób umożliwiający zrekonstruowanie i zbadanie ewolucji systemu cząstek w przestrzeni i czasie \cite{aliceofficial}. 

\subsubsection{Eksperyment}
W ekstremalnych warunkach temperatury i (lub) gęstości materia hadronowa topi się w osoczu wolnych kwarków i gluonów --- tak zwanej plazmy kwarkowo-gluonowej (ang. quark–gluon plasma --- QGP). Aby stworzyć odpowiednie warunki w laboratorium, ciężkie jony (np. cząstki ołowiu) przyspiesza się do niemal prędkości światła, po czym jest powodowana kolizja. Kluczowym rozważaniem dotyczącym eksperymentu ALICE jest zdolność do badania chromodynamiki kwantowej (ang.  quantum chromodynamics --- QCD) i kwarków w tych ekstremalnych warunkach. Odbywa się to przy użyciu cząstek, utworzonych wewnątrz gorącej objętości podczas jej rozszerzania się i ochładzania. Cząstki te żyją wystarczająco długo, aby dotrzeć do wrażliwych warstw detektora zlokalizowanych wokół obszaru oddziaływania.

 Fizyka w ALICE polega na tym, żeby być w stanie zidentyfikować wszystkie cząstki (tj. określić, czy są to elektrony, fotony, piony itd.), czy też określić ich ładunek. Wiąże się to w większości z różnymi sposobami oddziaływania cząstek z materią \cite{aliceexperiment}.

\subsubsection{Ślady cząstek i detektory}
Zespół detektorów cylindrycznych (od wewnątrz na zewnątrz: ITS\footnote{The Inner Tracking System}  Drift, ITS Strips, TPC\footnote{The ALICE Time Projection Chamber}, TRD\footnote{The ALICE Transition Radiation Detector}) mierzy w wielu punktach (ponad 100 tylko dla TPC) przejście każdej cząstki przenoszącej ładunek elektryczny tak, że trajektoria jest dokładnie znana. Detektory ALICE są osadzone w polu magnetycznym (wytwarzanym przez duży czerwony magnes), wyginając w ten sposób trajektorie cząstek: z krzywizny śladów można znaleźć ich pęd. ITS jest tak precyzyjny, że cząstki, które są generowane przez rozkład innych cząstek o bardzo krótkim czasie życia można zidentyfikować, widząc, że nie pochodzą one z punktu, w którym nastąpiła interakcja ("wierzchołek"\ zderzenia) \cite{trackingparticles}.

\begin{figure}[H]
		\centering
 		\includegraphics[width=16.0cm]{detector.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Detektor ALICE \cite{aliceofficial}.}
 		\label{rys9}
\end{figure}
%\subsubsection{Geometria detektora}
\newpage
\section{Projekt oprogramowania}
Celem oprogramowania w niniejszej pracy dyplomowej jest wsparcie badania stereoskopowych wizualizacji oraz tworzenia realistycznych obrazów torów cząstek i detektora ALICE. 

Założono, że program będzie wykonany w języku C++ z użyciem biblioteki OpenGL i bibliotek towarzyszących, takich jak na przykład  GLFW czy GLM.

Na etapie analizy możliwych rozwiązań i technologii wykryto, że pobieranie danych o zderzeniach cząstek i o geometrii detektora bezpośrednio z CERN byłoby zbyt kosztowne czasowo, dlatego zostało zgłoszone zapotrzebowanie w plikach dostarczających te informacje. Dla przechowywania geometrii detektora wybrano format danych Collada, co wynikało z wcześniejszego przestudiowania dostępnych formatów i wybrania najbardziej dostosowanego do zadania. Dane o torach cząstek muszą być dostarczone w formacie Json, gdyż jest to ustrukturyzowany format pozwalający na łatwy zapis i odczyt informacji. W związku z powyższymi uwagami wiąże się wybór dodatkowych bibliotek wspierających program: JsonCpp, dla parsowania plików Json przechowujących dane o zderzeniach, i Assimp, dla wczytania modelu.

W częsci analitycznej zostały wytypowane 2 techniki stereoskopowe dla dalszego przeanalizowania i wykorzystania w oprogramowaniu: stereoskopia pasywna i technika stereoskopii aktywnej. Obie metodyki wymagają specjalistycznego sprzętu. Tak w przypadku technologii pasywnej jest to zestaw: ekran 3D i specjalnie spolaryzowane okulary, natomiast dla stereoskopii aktywnej jest to kompletny zestaw okularów Nvidia 3D Vision i emitera Nvidia IR 3D Vision z interfejsem USB. Te zestawy zostały udostępnione na cele badawcze w wydziałowym laboratorium grafiki komputerowej.

Dla ułatwienia użytkowania programu sterowanie widokiem będzie się sprowadzało do ruchów myszą i wciśnięcia określonych klawiszy. To ograniczenie w dużym stopniu redukuje niebezpieczeństwo wprowadzenia niepoprawnych danych, tym samym zmniejszając prawdopodobieństwo nieprzewidzianego zachowania programu.

Produktem końcowym jest program poprawnie tworzący i wyświetlający obrazy zgodnie z dostarczonymi danymi, który odpowiednio reaguje na działania użytkownika. Weryfikacja poprawności działania oprogramowania będzie polegała głównie na sprawdzeniu eksperymentalnym czy stworzone obrazy rzeczywiście spełniają kryteria obrazów stereoskopowych. 

\subsection{Wymagania funkcjonalne}
Zanim podjęto bezpośrednią pracę nad programem sprecyzowano wymagania, jakie powinien spełniać końcowy produkt. Wymagania funkcjonalne wraz z opisami są spisane w tabeli \ref{tab2}. Podane zostały również priorytety każdego z wymagań (1 dla zadań najważniejszych) wyróżnające ważność wykonania zadania w całości. 

\begin{table}[H]
\caption{Wymagania fukcjonalne.}
\centering
\footnotesize
\label{tab2}
\begin{tabular}{!{\color{sapphire}\vrule width 1pt}m{0.05\textwidth}!{\color{black}\vrule width 1pt}m{0.26\textwidth}!{\color{black}\vrule width 1pt}m{0.50\textwidth}!{\color{black}\vrule width 1pt}m{0.07\textwidth}!{\color{sapphire}\vrule width 1pt}}
	\arrayrulecolor{sapphire}\hline
	\Centering\bfseries Nr &
	\Centering\bfseries Nazwa &
	\Centering\bfseries Opis &
	\Centering\bfseries Priorytet \\
	\hline
	\arrayrulecolor{black}
	1 & Renderowanie torów cząstek & Program wyświetla ślady cząstek, odczytując kształt krzywych z położenia punktów kontrolnych z dostarczonego pliku JSON. & 1 \\ 
	\hline
	1.1 & Poprawne kolorowanie śladów & Tory są poprawnie pokolorowane w zależności od masy cząstek. Dane są pobierane z dostarczonego pliku JSON. & 1 \\
	\hline
	2 & Renderowanie detektora & Program wyświetla detektor ALICE. Geometria detektora jest dostarczona w postaci pliku o rozszerzeniu .dae. Program odczytuje niezbędne dane z pliku, przetwarza je i wyświetla obraz na ekranie. & 1 \\ 
	\hline
	3 & Stworzenie efektu widzenia stereoskopowego & Program umożliwia oglądanie wyrenderowanych obrazów przy pomocy stereoskopii pasywnej. & 1 \\ 
	\hline
	4 & Stworzenie efektu aktywnego widzenia stereoskopowego & Program umożliwia oglądanie wyrenderowanych obrazów przy pomocy techniki aktywnego widzenia stereoskopowego. & 1 \\ 
	\hline
	5 & Możliwość interakcji z użytkownikiem & Program umożliwia użytkownikowi łatwe sterowanie położeniem oraz obrotami kamery. Funkcjonalność jest dostępna zarówno przy oglądaniu zwykłym jak i stereoskopowym. & 2 \\ 
	\arrayrulecolor{sapphire}\hline
\end{tabular}
\end{table}

\subsection{Wymagania niefunkcjonalne}
Wskazanie funkcji oprogramowania nie określa wszystkich cech istotnych dla użytkownika \cite{specyfikacja}. Wymagania niefunkcjonalne, które w danym zastosowaniu są ważne, zostały określone i opisane w tabeli \ref{tab3}. Ze względu na badawczą specyfikę niniejszego programu zostały pominięte niektóre zalecane wymagania niefunkcjonalne. Te zaś, które umieszczono poniżej, spriorytezowano podobnie jak w przypadku wymagań funkcjonalnych.

\begin{table}[H]
\caption{Wymagania niefukcjonalne.}
\centering
\footnotesize
\label{tab3}
\begin{tabular}{!{\color{sapphire}\vrule width 1pt}m{0.05\textwidth}!{\color{black}\vrule width 1pt}m{0.26\textwidth}!{\color{black}\vrule width 1pt}m{0.50\textwidth}!{\color{black}\vrule width 1pt}m{0.07\textwidth}!{\color{sapphire}\vrule width 1pt}}
	\arrayrulecolor{sapphire}\hline
	\Centering\bfseries Nr &
	\Centering\bfseries Nazwa &
	\Centering\bfseries Opis &
	\Centering\bfseries Priorytet \\
	\hline
	\arrayrulecolor{black}
	1 & Przenośność & Program działa bez błędów na różnych systemach operacyjnych (Windows, Linux). W systemie operacyjnym Linux dopuszczalny jest brak funkcjonalności aktywnego widoku stereoskopowego ze względu na zewnętrzne urządzenia. & 1 \\ 
	\hline
	2 & Zrozumiałość & Program jest łatwy w użyciu. & 1 \\ 
	\hline
	3 & Czytelność kodu źródłowego & Kod źródłowy programu jest napisany zgodnie z zasadami, wzorcami i najlepszymi praktykami branży. & 1\\ 
	\hline
	4 & Tolerowanie defektów & Przewidywalne zachowanie programu pomimo pomyłek obsługi lub wprowadzenia błędnych danych. & 1 \\
	\hline
	5 & Przyjazność programu dla użytkownika & Program jest intuicyjny w obsłudze. & 2\\  
	\arrayrulecolor{sapphire}\hline
\end{tabular}
\end{table}

\newpage
\subsection{Przypadki użycia}
Wymagania użytkownika programu mogą być pokazane jak lista zadań, które może być wykonana z użyciem rozwiajanego programu. W takim przypadku każde takie zadanie musi być opisane, z uwzględnieniem kolejności poszczególnych kroków, które by doprowadziły użytkownika do konkretnego miejsca w programie. Podsumowując, opis wymagań sprowadza się do opisu możliwych sposobów używania programu przez aktorów (użytkowników).

Aktorem w danym kontekście jest użytkownik programu. Przez to, że głównym celem tej oto pracy dyplomowej są wizualizacje, wszystkie przypadki użycia sprowadzają się do oglądania wyrenderowanych scen oraz poruszania się w nich.

\begin{table}[H]
\caption{Przypadki użycia programu}
\centering
\footnotesize
\label{tab4}
\begin{tabular}{!{\color{sapphire}\vrule width 1pt}m{0.05\textwidth}!{\color{black}\vrule width 1pt}m{0.16\textwidth}!{\color{black}\vrule width 1pt}m{0.50\textwidth}!{\color{black}\vrule width 1pt}m{0.17\textwidth}!{\color{sapphire}\vrule width 1pt}}
	\arrayrulecolor{sapphire}\hline
	\Centering\bfseries Nr &
	\Centering\bfseries Nazwa &
	\Centering\bfseries Opis &
	\Centering\bfseries Scenariusz alternatywny \\
	\hline
	\arrayrulecolor{black}
	FU1 & Wyświetlanie torów cząstek & 
	\begin{itemize}
	\itemi Użytkownik uruchamia program.
	\itemi Wybiera opcję zwykłego wyświetlania torów cząstek.
	\itemi Steruje kamerą przy pomocy klawiatury i myszy.
	\end{itemize} & Brak \\ 
	\hline
	FU2 & Wyświetlanie detektora & \begin{itemize}
	\itemi Użytkownik uruchamia program.
	\itemi Wybiera opcję zwykłego wyświetlania detektora.
	\itemi Steruje kamerą przy pomocy klawiatury i myszy.
	\end{itemize} & Brak \\ 
	\hline
	FU3 & Stereoskopowe wyświetlanie obrazów & \begin{itemize}
	\itemi Użytkownik uruchamia program.
	\itemi Wybiera opcję stereoskopowego wyświetlania detektora (lub torów cząstek).
	\itemi Przełącza ekran w tryb 3D oraz zakłada specjalne okulary.
	\itemi Steruje kamerą przy pomocy klawiatury i myszy.
	\end{itemize} & Brak \\ 
	\hline
	FU4 & Aktywne wyświetlanie obrazów stereoskopowych & \begin{itemize}
	\itemi Użytkownik uruchamia program.
	\itemi Wybiera opcję aktywnego wyświetlania obrazu stereoskopowego detektora (lub torów cząstek).
	\itemi Przełącza ekran w tryb aktywnego widzenia 3D oraz zakłada specjalne okulary.
	\itemi Steruje kamerą przy pomocy klawiatury i myszy.
	\end{itemize} & Brak \\ 
	\hline
	FU5 & Przełączenie widoków & \begin{itemize}
	\itemi Użytkownik uruchamia program.
	\itemi Wybiera opcję zwykłego wyświetlania torów cząstek.
	\itemi Wybiera opcję stereoskopowego wyświetlania detektora.
	\itemi Przełącza ekran w tryb 3D oraz zakłada specjalne okulary.
	\itemi Wybiera opcję zwykłego wyświetlania obrazu detektora. 
	\itemi Zejmuje okulary 3D, przełącza ekran w tryb zwykłego wyświetlania.
	\itemi Steruje kamerą przy pomocy klawiatury i myszy.
	\end{itemize} & Brak \\ 
	\arrayrulecolor{sapphire}\hline
\end{tabular}
\end{table}


\subsection{Diagram klas}
Model przypadków użycia opisuje zachowanie programu z punktu widzenia zwykłego użytkownika, nie określa on natomiast sposobu, w jaki
należy oprogramowanie stworzyć. Podstawowymi elementami programu napisanego w języku obiektowym są klasy. Mogą one być powiązane z innymi klasami programu na wiele różnych sposóbów. Nieduży rozmiar programu pozwala na projektowanie architektury na poziomie klas.

Na tym etapie projektu diagram klas jest wciąż postrzegany tylko w perspektywie pojęciowej, gdzie nie podjęto jeszcze decyzji o dokładnym rozmieszczeniu w klasach metod implementujących przypadki użycia opisane w tabeli \ref{tab4}. Diagram na rysunku \ref{rys24} przedstawia wstępną wersję budowy klas w programie. 

\begin{figure}[H]
		\centering
 		\includegraphics[width=15.5cm]{diagramKlas.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Wstępny diagram klas programu wizualizacji.}
 		\label{rys24}
\end{figure}

Pisanie, kompilowanie i zarządzanie modułami cieniującymi może być dość kłopotliwe. Dlatego postanowiono zebrać te metosy do wspolnej klasy klasy Shader (moduł cieniujący). Ta klasa miałaby za zadanie odczytanie modułów z dysku, ich kompilację i łączenie, sprawdzałaby także błędy występujące w trakcie przetwarzania modułów cieniujących.

Klasa Camera (kamera) odpowiadałaby za zmiany widoku przy interakcji z użytkownikiem. Zmiany widoku by zachodziły po wciśnieciu wybranych klawiszy, po poruszeniu myszą czy kółkiem myszy. Dodatkowo ta klasa obliczałaby widok stereoskopowy (computeStreoView), innymi słowy tworzyłaby dwa obrazy z różniącą się perspektywą jako materiał stereoskopowy. Wartość wyliczeniowa Camera\_Movement (ruch kamery) określałaby kierunek ruchu kamery w zależności od wciśniętego przycisku. Ułatwi to komunikację pomiędzy obsługą zdarzeń przychodzących z kalwiatury a kamerą, owocując w odpowiednie przeliczania widoku w programie. 

W klasie Camera również zostaną zdefiniowane takie zmienne globalne jak szybkość poruszania się kamery, wartości kątów obrotów czy wrażliwość kamery na ruchy myszy.

Klasy Mesh (siatka) i Model są musza być opracowane na potrzeby wczytania modelu z pliku o formacie Collada do programu. Konieczna będzie do zrealizowania zmiana struktur danych z biblioteki Assimp na konstrukcje obsługiwane przez OpenGl. Obiekt typu Mesh będzie przechowywał odczytane informacje o poszczególnej siatce, natomiast obiekt typu Model będzie gromadził dane o modelu jako całości. Dla poprawnego wyświetlenia modelu detektora wczytanego do obiektu typu Model zaprojektowano połączenie Model-Shader.

W klasie Track wczytywano by dane z pliku JSON. Po czym byłyby aktualizowane bufory przesyłające informację prosto do karty graficznej. Ten krok przyspieszyłby generowanie obrazów nie wywierając negatywnych skutków na obrazach, przedstawiających tory cząstek po kolizji w detektorze.

\newpage
\subsection{Technologie}
\subsubsection{Dodatkowe biblioteki graficzne}
Biblioteka ładująca OpenGL (ang. OpenGL Loading Library) wciąga wskaźniki do funkcji OpenGL w środowisku wykonawczym, rdzeniu i rozszerzeniach. Jest to wymagane, aby uzyskać dostęp do funkcji OpenGL w wersjach powyżej 1.1 na większości platform. Duża liczba bibliotek ładujących rozszerzenia zastępuje potrzebę włączenia gl.h. Zamiast tego udostępniają własny nagłówek, który musi zostać użyty. Większość bibliotek ładowania używa specjalnego generatora do skonstruowania kodu, który wciąga wskaźniki funkcji i zawarte nagłówki \cite{LoadingLibrary}. 

GLAD --- biblioteka generująca kod do ładowania GL. Za pomocą GLAD można wygenerować nagłówki lub kod źródłowy dla dowolnej wersji OpenGL, od wersji 1.1 do 4.6. Można również dołączyć dowolne rozszerzenia OpenGL.

GLFW --- biblioteka dla OpenGL rozwijana jako opragramowanie otwarte (ang. open source). GLFW zapewnia łatwość w tworzeniu okien i kontekstów, w obsłudze danych i zdarzeń wejściowych. Biblioteka jest napisana w języku C, dzięki czemu jest multiplatformowa.  

Obsługę wielu okien, monitorów, ramp o wysokiej rozdzielczości DPI i gamma wymienia się jako jedną z najwiekszych zalet tej biblioteki. Oprócz tego, poprzez odpytywania obsługiwane są klawiatura, mysz, gamepad, czas oraz okna zdarzeń, co jest bardzo wygodne w zarządzaniu podczas tworzenia wizualicji. W dodatku GLFW ma dostęp do rodzimych obiektów i opcji kompilacji dla specyficznych funkcji platform \cite{glfw}.

GLEW (ang. OpenGL Extension Wrangler Library, biblioteka rozszerzeń OpenGL) --- wieloplatformowa biblioteka w C (lub C ++). GLEW zapewnia wydajne mechanizmy określające, które rozszerzenia OpenGL są obsługiwane na platformie docelowej. Rdzeń OpenGL oraz funkcje rozszerzenia są ujawnione w pojedynczym pliku nagłówkowym. Biblioteka została przetestowana na różnych systemach operacyjnych, w tym Windows, Linux, Mac OS X i Solaris \cite{glew}.

GLM (ang. OpenGL Mathematics, matematyka OpenGL) --- nagłówek do biblioteki matematycznej C++, opracowany dla oprogramowania graficznego opartego na specyfikacjach języka GLSL. GLM udostępnia klasy i funkcje zaprojektowane i zaimplementowane z użyciem tej samej konwencji nazewnictwa jak w GLSL. Biblioteka zapewnia rozszerzone możliwości: przekształcenia macierzy i kwaternionów, pakowanie danych, liczby losowe, szumy itd. GLM doskonale działa z OpenGL, nadaje do oprogramowania renderowania i rasteryzacji, przetwarzania obrazu, symulacji zjawisk fizycznych oraz wszelkich kontekstów rozwojowych, które wymagają prostej i wygodnej biblioteki matematycznej \cite{glm}. 

\subsubsection{Glitter}
Założono, że projekt będzie łatwy w użyciu, lecz czasami instalowanie zależności może być bardzo frustrujące, szczególnie w środowiskach, w których brakuje zarządcy pakietów lub uprawnień administracyjnych, dlatego pierwsze skonfigurowanie programu może być dużym wyzwaniem.
Dla zapewnienia przenośności oraz łatwości użycia opracowywanego programu skorzystano z gotowego prostego zestawu dla OpenGL pt. Glitter.

Glitter kompiluje i statycznie łączy wszystkie niezbędne biblioteki. Robi on to za pośrednictwem jednej zależnośći: cmake, która służy do generowania specyficznych dla platformy plików makefile lub plików projektu. Glitter łączy większość zależności potrzebnych do wdrożenia podstawowego mechanizmu renderującego \cite{glitter}, zawiera biblioteki:
\begin{itemize}
\itemi assimp
\itemi glfw
\itemi glm
\itemi glad
\end{itemize}

\subsubsection{JSONcpp}
JsonCpp --- to biblioteka w języku C++ umożliwiająca manipulowanie wartościami JSON, w tym serializację i deserializację ciągów. Dzięki JsonCpp istenieje możliwość odczytać i zapisać plik formatu JSON, dołączyć komentarze w konwencji C++ do elementu podczas parsowania oraz przerobić plik JSON zachowując oryginalne komentarze \cite{jsoncpp}.

Jednym ze sposobów integracji JsonCpp z projektem jest dołączenie plików z kodem źródłowym (pojedynczego pliku .cpp i dwóch plików .h) do projektu oraz ich kompilacja w taki sam sposób, jak każdy inny plik źródłowy. Zapewnia to spójność flag kompilacji i kompatybilności, rozwiązuje problemy powstające podczas budowania bibliotek współdzielonych lub statycznych. 

\subsubsection{Google Test}
Google Test --- to biblioteka do tworzenia testów jednostkowych w C++, może byc skompikowana na różnych platformach (m.in. Posix, Windows). Testy mogą być uruchamiane pojedynczo lub wszystkie naraz. To sprawia, że proces szukania i naprawiania jest bardzo łatwy i intuicyjny\cite{googleTest}.

Spośród wielu bibliotek do testowania jesdnostkowego została wybrana ta, ponieważ przeciwieństwie do wielu innych bibliotek, struktura testowa Google ma wbudowane asercje. Można je wdrażać do oprogramowania, w którym obsługa wyjątków jest wyłączona (zazwyczaj ze względu na wydajność). Tak więc asercje mogą być również bezpiecznie stosowane w destruktorach \cite{ibmGoogle}.

Przeprowadzanie testów jest proste. Wystarczy nawiązać połączenie z wcześniej zdefiniowanym makrem RUN\_ALL\_TESTS, wtedy gdy w innych bibliotekach musi być stworzona lub wprowadzona oddzielna klasa uruchamiająca wykonanie testu. 
\newpage
\section{Część weryfikacyjna}
\subsection{Macierze modelu, widoku, projekcji}
Obraz wyświetlany na ekranie --- to wierzchołki o konkretnych współrzędnych dwuwymiarowych przedstawiające scenę 3D, lecz początkowo scenę tworzy się w przestrzeni trójwymiarowej. Przejścia z jednego układu współrzędnych na inny są znacznie wygodniejsze przy użyciu macierzy transformacji: modelu, widoku oraz projekcji. 

Lokalny układ współrzędnych to taki układ, w którym obiekt ma swój początek. Położenia wierzchołków obiektu są zmieniane względem początku układu lokalnego. Lokalne układy współrzędnych są przeliczane na współrzędne świata, dzięki temu obiekty są rozmieszczane w różnych miejscach sceny. W kolejnych krokach układ współrzędnych świata jest przekształcany na widok z perspektywy kamery. Po wprowadzeniu współrzędnych do widoku są one zmieniane na współrzędne rzutni. Decydującym faktem jest to, że współrzędne rzutni są przetwarzane w zakresie (-1.0; 1.0), określając jednoznacznie wierchołki, które mają być wyświetlone na ekranie. Ostateczny obraz powstaje po przeliczeniu współrzędnych rzutni na współrzędne ekranu, które są następnie przesyłane do rasteryzatora. Opisane prześcia i transformacje macierzy są zilustrowane na rys.\ref{rys11}.
\begin{figure}[H]
		\centering
 		\includegraphics[width=12.0cm]{coordinate_systems.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Przejścia z jednego układu współrzędnych na inny \cite{opengltutorial}.}
 		\label{rys11}
\end{figure}
Stąd przyjęto założenie, że wszystkie wierzchołki modelu są w lokalnym układzie współrzędnych obiektu. Natomiast macierz modelu reprezentuje  obiekt w układzie współrzędnych świata, ze względu na możliwość skalowania, obrotu czy translacji. Widok jest z reguły przypisywany do kamery. Macierz projekcji początkowo przypisywana jest do przestrzeni rzutni w zakresie (-1000; 1000) na każdej osi, lecz ostatecznie jest normalizowana do zakresu (-1.0; 1.0). Wszystkie wierzchołki umieszczone poza tym zakresem nie będą widoczne na ekranie.

\subsection{Kamera}
Jednym z wymagań funkcjonalnych było sterowanie kamerą w celu przesunięcia bądź przybliżenia widoku. Umożliwia to plik nagłówkowy Camera.h.

Podchodząc do rozwiązania takiego zadania zakłada się, że za pomocą macierzy widoku wszystkie współrzędne sceny są przekształcane na współrzędne widoku względem położenia i kierunku patrzenia kamery. Stąd do zdefiniowania kamery niezbędne są: 
\begin{itemize}
\itemi położenie kamery w scenie;
\itemi kierunek patrzenia kamery;
\itemi wektora skierowanego w prawo od kamery;
\itemi wektora skierowanego w górę od kamery.
\end{itemize}
W ten sposób został utworzony układ współrzędnych, gdzie położenie kamery jest punktem początkowym, a trzy pozostałe wektory --- to prostopadłe do siebie osie. 

Funkcja glm::LookAt z biblioteki GLM przyjmuje jako argumenty położenie kamery, kierunek patrzenia oraz wektor skierowany w górę (tworzy z tego wektor skierowany w prawo od kamery), zwracając przy tym gotową macierz widoku. W głównym pliku programu została zdefiniowana funkcja processInput, która używając biblioteki GLFW zarządza danymi przychodzącymi z klawiatury. Dzięki tej funkcji łączone są określone klawisze z odpowiednimi zmianami położenia kamery.

Obroty kamery zostały zrealizowane przy użyciu macierzy obrotu i funkcji trygonometrycznych. Wartości odchylenia i nachylenia kamery są pozyskiwane z ruchu myszy, gdzie poziomy ruch myszy wpływa na odchylenie, a pionowy odpowiednio na nachylenie. Zapamiętywana jest pozycja myszy w ostatniej klatce, a w aktualnej jest obliczana różnica wartości aktualnych i zapamiętanych. W zależności od wielkości obliczonych wartośći porusza się kamera. Dla uniknięcia zbyt szybkich albo dużych odchyleń (lub nachyleń) zostały wprowadzone ograniczniki, zmniejszające zakres poruszania kamerą \cite{learnopengl}. 

Również wprowadzono do programu obsługę przybliżenia. Rozwiązano to używając właściwości "pola widzenia"\ : jak pole widzenia staje się mniejsze, rzutowana przestrzeń sceny zmniejsza się, dając iluzję powiększania. Do sterowania powiększaniem używa się kółka myszki.

\newpage
\subsection{Rysowanie ścieżek cząstek}
W celu narysowania ścieżek cząstek niezbędne są wierzchołki składające się na nie. Zgodnie z założeniem początkowym są one odczytywane z pliku JSON. Współrzędne odczytanych wierzchołków są zapisywane do wspólnej tablicy. Wraz ze ścieżkami są odczytywane masy cząstek, po których zostały ślady. Masa cząstki jest później używana do kolorowania toru.

\subsubsection{Odczytanie danych}
Proces odczytania danych ze specjalnie przygotowanego pliku JSON jest realizowany z użyciem biblioteki JsonCpp. W tabeli \ref{tab6} jest przedstawiony kawałek kodu źródłowego, który deserializuje dane JSON (Json::Reader readerTracks) i zapisuje odczytane wartości do tablicy dwuwymiarowej. Json::Value reprezentuje wartość JSON, zmienna ifs przekazuje ścieżkę do pliku do strumienia wejściowego (ang. ifstream, strumień wejściowy do obsługi plików).
\begin{table}[H]
\caption{Kod źródłowy programu. Odczytanie danych z pliku JSON.}
\label{tab6}
\begin{lstlisting}[frame=single]  % Start your code-block

Json::Value objTracks; 
Json::Reader readerTracks; 
// parsowanie dostarczonego pliku do obiektu typu Value;
readerTracks.parse(ifs,objTracks); 

for (Json::Value::iterator it = objTracks["fTracks"].begin(); 
 it != objTracks["fTracks"].end(); ++it){
  int TrackIndex = it.key().asInt();
  for (int i = 0; i < (*it)["fPolyX"].size(); i++){
   tracks[TrackIndex][i*5] = (*it)["fPolyX"][i].asFloat();
   tracks[TrackIndex][i*5 + 1] = (*it)["fPolyY"][i].asFloat();
   tracks[TrackIndex][i*5 + 2] = (*it)["fPolyZ"][i].asFloat();
   tracks[TrackIndex][i*5 + 3] = (*it)["fMass"].asFloat();
   tracks[TrackIndex][i*5 + 4] = 1.0f;
  }
\end{lstlisting}
\end{table}
Zdefiniowana w ten sposób tablica wierzchołków jest wysyłana na wejście modułu cieniującego wierzchołków dla dalszego przetwarzania. Odbywa się to poprzez zaalokowanie pamięci w procesorze graficznym, skonfigurowanie sposobu interpretowania tej pamięci przez OpenGL oraz określenie sposobu wysyłania danych do karty graficznej. Zatem moduł cieniujący wierzchołków przetwarza tylko taką liczbę punktów, jaka została zdefiniowana w pamięci.

\subsubsection{Inicjalizowanie buforów}
Moduł cieniujący wierzchołków pozwala na określenie dowolnych danych wejściowych w postaci atrybutów wierzchołków. Oznacza to, że poszczególne części danych wejściowych są ręcznie przypisywane do atrybutów wierzchołka. W takim przypadku potrzebne jest określenie, w jaki sposób OpenGL będzie interpretować dane przed renderowaniem. Założono, że:
\begin{itemize}
\itemi dane pozycji są przechowywane jako 32-bitowe (4 bajty) wartości zmiennoprzecinkowe;
\itemi każda pozycja składa się z 5 wartości: 3 współrzędnych położenia i 2 pozycji koloru;
\itemi nie ma pustych miejsc w tablicy, wszystkie wartości są obok siebie;
\itemi pierwsza wartość jest początkiem bufora.
\end{itemize}
Używając funkcji glEnableVertexAttribArray określono w jaki sposób dane wierzchołków będą interpretowane. Przykład implementacji powyższych założeń jest przedstawiony w tabeli \ref{tab7}.
\begin{table}[H]
\caption{Kod źródłowy programu. Przekazanie danych do VAO i VBO.}
\label{tab7}
\begin{lstlisting}[frame=single]  % Start your code-block

 glGenBuffers(1, &VBO[TrackIndex]);
 glGenVertexArrays(1, &VAO[TrackIndex]);
 glBindVertexArray(VAO[TrackIndex]);
 glBindBuffer(GL_ARRAY_BUFFER, VBO[TrackIndex]);
 glBufferData(GL_ARRAY_BUFFER, sizeof(tracks[TrackIndex]), 
  &tracks[TrackIndex], GL_STATIC_DRAW);
 glEnableVertexAttribArray(0);
 glVertexAttribPointer(0, 2, GL_FLOAT, GL_FALSE, 5 * sizeof(float), 0);
 glEnableVertexAttribArray(1);
 glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 5 * sizeof(float), 
  (void*)(2 * sizeof(float)));
 glBindVertexArray(0); }
\end{lstlisting}
\end{table}
Obiekt tablicy wierzchołków (ang. VAO --- Vertex Array Object) jest powiązany z obiektem bufora wierzchołków. Stąd wszelkie kolejne wywołania atrybutów wierzchołków będą przechowywane wewnątrz VAO. Ma to tę zaletę, że podczas konfigurowania wskaźników atrybutów wierzchołków wystarczy je wywołać tylko raz. Rysując obiekt można po prostu powiązać odpowiedni VAO, dzięki temu przełączanie pomiędzy różnymi konfiguracjami danych a konfiguracją wierzchołków jest bardzo proste.

Do przechowywania dużej liczby wierzchołków w pamięci GPU, a także do zarządzania tym zasobem posłużono się buforem wierzchołków obiektów (ang. VBO --- Vertex Buffer Objects). Zaletą tego rozwiązania jest bezpośrednie przesyłanie dużej ilości danych do karty graficznej. Gdy niezbędne dane już są w pamięci karty graficznej, moduł cieniujący wierzchołków ma praktycznie natychmiastowy dostęp do wierzchołków, dzięki czemu działa z wyjątkową szybkością.

\subsubsection{Moduły cieniujące}
Następnie były stworzone macierze transformacji. Zostały one przekazane do modułów cieniujących, tam zdefiniowano je jako uniformy i przemnożono ze współrzędnymi wierzchołków. Moduł cieniujący fragmentów jest dość prosty w tym przypadku, gdyż jedynie określa kolor ścieżki, która została po cząstce. Poniżej są umieszczone moduły cieniujące wierzchołków (tabela \ref{tab8}) oraz geometrii (tabela \ref{tab9}).
\begin{table}[H]
\caption{Kod źródłowy programu. Moduł cieniujący wierzchołków.}
\label{tab8}
\begin{lstlisting}[frame=single]  % Start your code-block

#version 420 core
layout (location = 0) in vec3 aPos;
layout (location = 1) in vec2 aColor;
out VS_OUT {
    vec2 color;
} vs_out;
uniform mat4 transform;
uniform mat4 view;
uniform mat4 projection;
void main(){
 vs_out.color = aColor;
 gl_Position = projection * view * transform * 
           vec4(aPos.x, aPos.y, aPos.z, 1.0); }
\end{lstlisting}
\end{table}
\begin{table}[H]
\caption{Kod źródłowy programu. Moduł cieniujący geometrii.}
\label{tab9}
\begin{lstlisting}[frame=single]  % Start your code-block

#version 420 core
layout (lines_adjacency) in;
layout (line_strip, max_vertices = 4) out;
in VS_OUT {
    vec2 color;
} gs_in[];
out vec2 fColor;
void main(void){
 fColor = gs_in[0].color;
 int i;
 for (i = 0; i < gl_in.length(); i++){
  gl_Position = gl_in[i].gl_Position;
  EmitVertex(); } }
\end{lstlisting}
\end{table}

Moduł cieniujący geometrii w przeciwieństwie do modułu cieniującego wierzchołków, posiada wszystkie dane określające prymityw. Dla każdego prymitywu wejściowego, moduł cieniujący geometrii ma dostęp jak do wszystkich jego wierzchołków tak i do informacji o sąsiedztwie, jeśli takie informacje zostały wcześniej określone. W przypadku opracowywanego programu na wejście modułu cieniującego geometrii zostały podane przylegające do siebie kawałki krzywej (ang. lines adjacency). Na wyjściu tego modułu otrzymano jedną spójną ścieżkę.

Pisanie, kompilowanie i zarządzanie modułami cieniującymi może być dość kłopotliwe, dlatego została zaimplementowana klasa Shader. Odczytuje ona moduły z dysku, kompiluje je, łączy oraz sprawdza błędy. 

\subsubsection{Antyalising}
Podczas tworzenia obrazów tórów cząstek napotkano problem zniekształcenia linii --- aliasing. Aliasing jest zjawiskiem związanym z rasteryzacją obiektów. Wszystkie wyświetlane prymitywy składają się z pewnej liczby pikseli, dlatego poszczególne punkty rastra są widoczne na urządzeniu końcowym. Poziom zniekształceń zależy od rozdzielczości ekranu. 

Techniki eliminujące efekt aliasingu, nazywane są antyaliasingiem (lub wygładzaniem krawędzi). Najczęściej polegają one na dodaniu pikseli uzupełniających brzegi figury lub procesie nadpróbkowania. Nadpróbkowanie sprowadza się do przeprowadzenia rasteryzacji w rozdzielczości większej niż docelowa, a następnie odpowiedniego skalowania. Biblioteka OpenGL udostępnia obie techniki antyaliasingu. 

W niniejszej pracy została wykorzystana metoda próbkowania wielokrotnego. Każdy piksel na krzywej został spróbkowany wiele razy. Dla każdego przebiegu próbki, zostało zastosowane nieduże przesunięcie wszystkich współrzędnych na ekranie. To przesunięcie jest odpowiednio mniejsze niż rzeczywisty rozmiar pikseli. Uśredniając wszystkie te próbki, otrzymuje się płynniejsze przejście kolorów \cite{glfw}. 

Antyaliasing linii w OpenGL wymaga tylko wywołania funkcji glEnable z parametrem GL\_MULTISAMPLE (wielopróbkowanie) oraz funkcji glfwWindowHint(GLFW\_SAMPLES, 8), gdzie GLFW\_SAMPLES określa żądaną liczbę próbek do wykorzystania w próbkowaniu. Podając liczbę 0 jako argument wyłącza się antyliasing. Liczba 8 została wybrana po kilku eksperymentach z różnymi wartościami tego parametru. Po próbach w laboratorium postanowiono użyć dokładniej tej liczby próbek. Obrazy z różnymi parametrami są przestawione na rysunkach \ref{rys12}, \ref{rys15}, \ref{rys16}.

Niestety nie da się w całości wyeliminować efektu aliasingu, lecz można znacznie zmniejszyć jego skutki poprzez zastosowanie różnych metod. Jak widać z obrazów \ref{rys13} po użyciu techniki wielokrotnego próbkowania krzywe ilustrujące tory cząstek się wygładziły, aczkolwiek wydają się być nieco grubsze.

\begin{figure}[H]
	\begin{subfigure}{0.45\textwidth}
		\centering
 		\includegraphics[width=\textwidth]{withAliasing.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Widoczny aliasing.}
 		\label{rys11}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Antialiasing.png}
		\captionsetup{font={up, footnotesize}}
    	\caption{Po zastosowaniu GLFW\_SAMPLES z wartością parametru 8.}
		\label{rys12}
	\end{subfigure}
	\captionsetup{justification=raggedleft, font={it, footnotesize}}
    \captionsetup{justification=justified, font={up, footnotesize}}
    \caption{Przykład zmniejszenia efektu aliasingu na wyrenderowanych torach cząstek w opracowywanym programie.}
    \label{rys13}
\end{figure}

\begin{figure}[H]
	\begin{subfigure}{0.5\textwidth}
		\centering
 		\includegraphics[width=\textwidth]{Wart3.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Po zastosowaniu GLFW\_SAMPLES z wartością parametru 3.}
 		\label{rys15}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Wart20.png}
		\captionsetup{font={up, footnotesize}}
    	\caption{Po zastosowaniu GLFW\_SAMPLES z wartością parametru 20.}
		\label{rys16}
	\end{subfigure}
	\captionsetup{justification=raggedleft, font={it, footnotesize}}
    \captionsetup{justification=justified, font={up, footnotesize}}
    \caption{Przykład zmniejszenia efektu aliasingu na wyrenderowanych torach w przypadku dużej liczby cząstek.}
    \label{rys17}
\end{figure}

%Na każdym z umieszczonych obrazów liczba torów jest inna, jak również widoczne są ścieżki o różnych kolorach (wartość określana z masy cząstki). 
%4 obrazy z różnych plików

\newpage
\subsection{Rysowanie detektora}
\subsubsection{Wczytanie danych z pliku Collada}
Do wczytania modelu z dostarczonego pliku w formacie Collada użyto biblioteki Assimp. Odbywa się to poprzez ładowanie wszystkich informacji o modelu do uogólnionych struktur danych zdefiniowanych w bibliotece. Po załadowaniu modelu, wszystkie niezbędne do renderowania dane mogą być pobrane ze struktur danych Assimp. Podczas opracowywania programu korzystano z samouczek opublikowanych na stronie \cite{learnopengl}. 

Pobierany z pliku model jest ładowany do obiektu sceny przechowującego wszystkie informacje o modelu czy scenie. Na rysunku \ref{rys14} jest pokazany diagram klas bezpośrednio dotyczący wczytanej sceny.
\begin{figure}[H]
		\centering
 		\includegraphics[width=10.0cm]{assimpClasses.jpg}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Diagram klas sceny wczytanej za pomocą biblioteki Assimp \cite{assimpImport}.}
 		\label{rys14}
\end{figure}

Klasa aiScene zawiera wskaźnik do aiNode (węzła głównego sceny), który z kolei utrzymuje wskaźniki do węzłów potomnych. Wskaźnik do aiNode rekursywnie definiuje strukturę sceny. AiScene służy również jako centralne miejsce przechowywania wszystkich siatek, materiałów lub źródeł światła. Natomiast instancje aiNode po prostu indeksują te tablice, które znajdują się w scenie. Obiekt mMesh przechowuje wszystkie istotne dane wymagane do renderowania: położenia wierzchołków, wektory normalnych, współrzędne tekstur, powierzchnie i materiały obiektu.

\subsubsection{Określenie siatki detektora}
Używając biblioteki Assimp istnieje możliwość załadowania wielu różnych modeli do jednej aplikacji, lecz po wczytaniu wszystkie dane są przechowywane w strukturach Assimp. Dla umożliwienia renderowania obiektów konieczne jest przekształcenie sruktur danych Assimp na struktury w OpenGL. 

W rozpatrywanym przypadku pojedyńcza encja do narysowania jest reprezentowana przez siatkę (ang. mesh), dlatego rozbudowanie programu zaczęto od zdefiniowania własnej klasy siatki. Składa się ona z zestawu wierzchołków, gdzie każdy wierzchołek zawiera wektory położenia, normalnych.

Klasa Mesh nie jest zbyt skomplikowana. W konstruktorze są podawane wszystkie potrzebne dane, funkcja setupMesh inicjalizuje bufory VAO i VBO. Siatka jest rysowana za pomocą funkcji Draw. Funkcja konstruktora jest dość prosta: publiczne zmienne klasy są ustawiane  z odpowiednimi zmiennymi argumentów konstruktora, na koniec jest wywoływane inicjalizowanie buforów --- funkcja setupMesh. 
\begin{table}[H]
\caption{Kod źródłowy programu. Funkcja inicjalizacji buforów.}
\label{tab12}
\begin{lstlisting}[frame=single]  % Start your code-block

void setupMesh() {
 glGenVertexArrays(1, &VAO);
 glGenBuffers(1, &VBO);
 glGenBuffers(1, &EBO);

 glBindVertexArray(VAO);
 glBindBuffer(GL_ARRAY_BUFFER, VBO);
 glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(Vertex), 
 	&vertices[0], GL_STATIC_DRAW);
 glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);
 glBufferData(GL_ELEMENT_ARRAY_BUFFER, 
 	indices.size() * sizeof(unsigned int),&indices[0], 
 	GL_STATIC_DRAW);
 glEnableVertexAttribArray(0);
 glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), 
 	(void*)0);
 glEnableVertexAttribArray(1);
 glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex),
 	(void*)offsetof(Vertex, Normal));
 glBindVertexArray(0);}
\end{lstlisting}
\end{table}

W kodzie źródłowym z tabeli \ref{tab12} użyto dodatkowego bufora --- EBO (ang. Element Buffer Objects) --- bufora elementów obiektu. W modelach przybliżanych siatką trójkątów duża część wierzchołków może się pokrywać. Lepszym rozwiązaniem jest przechowywanie nie wszystkich wierzchołków, a tylko unikatowych, wraz z określaniem kolejności, w której zostaną one wyświetlone \cite{learnopengl}. Wsparciem właśnie tej techniki jest EBO. Bufor elementów obiektu przechowuje indeksy używane przez OpenGL do podjęcia decyzji, które wierzchołki muszą być wyświetlone.

\subsubsection{Wyświetlanie modelu}
Scena może może zawierać wiele różnych siatek, dlatego utworzono klasę, która by reprezentowała model w całości. Z tego powodu w klasie Model został zdefiniowany wektor obiektów typu Mesh --- wektor skłądający się z różnych siatek odczytanych z pliku. Ładowanie danych z pliku odbywa się za pomocą funkcji loadModel, kod źródłowy tej funkcji jest przedstawiony w tabeli \ref{tab13}.

\begin{table}[H]
\caption{Kod źródłowy programu. Funkcja pobierająca model z pliku.}
\label{tab13}
\begin{lstlisting}[frame=single]  % Start your code-block

void loadModel(string const &path) {
 Assimp::Importer importer;
 const aiScene* scene = importer.ReadFile(path, aiProcess_Triangulate |
   aiProcess_FlipUVs | aiProcess_CalcTangentSpace | 
   aiProcess_JoinIdenticalVertices | aiProcess_SortByPType);

 if(!scene || scene->mFlags & AI_SCENE_FLAGS_INCOMPLETE 
    	|| !scene->mRootNode) {
  
  cout << "ERROR::ASSIMP::" << importer.GetErrorString() << endl;
  return; }
 directory = path.substr(0, path.find_last_of('/'));
 processNode(scene->mRootNode, scene); }
\end{lstlisting}
\end{table}

Początkowo jest tworzony obiekt Importer dedykowany do wczytywania modeli do struktur danych Assimp. Na tym obiekcie wywołano funkcję ReadFile, gdzie ścieżka do pliku Collada jest podawana jako pierwszy argument. Jako drugi argument poprzez funkcję ReadFile są przyjmowane różne opcje używane w trakcie przetwarzania końcowego. Opcje te inicjują dodatkowe obliczenia na odczytanych danych. Ze wszystkimi możliwymi opcjami oraz ich opisami można zapoznać się na stronie z oficjalną dokumentacją biblioteki \cite{assimpDocumentation}.

Jak było pokazane na diagramie klas (rysunek \ref{rys14}) scena w Assimp ma strukturę drzewiastą. Po wczytaniu sceny przetwarzane są wszystkie jej węzły, służy do tego rekursywna funkcja processNode, która zatrzymuje swoje działanie dopiero w chwili, gdy wszystkie węzły sceny zostały już przetworzone. Na tym etapie zostaje uzupełniony w dane wektor siatek.

Samo przetwarzanie siatki sprowadza się do 2 czynności: do pobierania wszystkich informacji o wierzchołkach oraz do pobierania indeksów siatki. Na koniec wywoływany jest konstruktor obiektu siatka. W tabeli \ref{tab14} z kodem źródłowym pokazana jest funkcja przetwarzająca siatki w modelu.

\begin{table}[H]
\caption{Kod źródłowy programu. Funkcja przetwarzania siatek w modelu.}
\label{tab14}
\begin{lstlisting}[frame=single]  % Start your code-block

Mesh processMesh(aiMesh *mesh, const aiScene *scene) {
 vector<Vertex> vertices;
 vector<unsigned int> indices;
 for(unsigned int i = 0; i < mesh->mNumVertices; i++) {
  Vertex vertex;
  glm::vec3 vector;
  vector.x = mesh->mVertices[i].x;
  vector.y = mesh->mVertices[i].y;
  vector.z = mesh->mVertices[i].z;
  vertex.Position = vector;

  vector.x = mesh->mNormals[i].x;
  vector.y = mesh->mNormals[i].y;
  vector.z = mesh->mNormals[i].z;
  vertex.Normal = vector;
  
  vertices.push_back(vertex);}
  
 for(unsigned int i = 0; i < mesh->mNumFaces; i++) {
  aiFace face = mesh->mFaces[i];
  for(unsigned int j = 0; j < face.mNumIndices; j++)
   indices.push_back(face.mIndices[j]); }
 return Mesh(vertices, indices);}
\end{lstlisting}
\end{table}

Pobieranie informacji o wierzchołkach zostało zrealizowane poprzez zdefiniowanie struktury wierzchołków, która po każdej iteracji jest dodawana do ogólnej tablicy wierzchołków. Do załadowania danych o indeksach siatki skorzystano z interfejsu udestępnionego przez bibiliotekę Assimp --- aiFace. Face, w kontekście opracowywanego programu, jest najmniejszym zdefiniowanym prymitywem i ma on kształt trójkąta. Ten właśnie prymityw przechowuje indeksy definiujące kolejność wyświetlania wierzchołków siatki.

Na podstawie uzyskanych danych z Assimp --- listy wierzchołków i indeksów --- uruchomiono pętlę rysującą siatkę. Do rysowania użyto funkcji glDrawElements, tak samo jak w przypadku torów cząstek.

\subsubsection{Napotkane problemy}
Przed rozpoczęciem prac nad programem wizualizacji detektora zakładano, że geometria modelu zostanie bezpośrednio pobrana z repozytorium w CERN. Okazało się jednak, że nie są to szeroko dostępne dane oraz że są one przechowywane w strukturach danych języka Root. Root --- język wewnętrznego użytkowania w CERN, nieco przypomina C, lecz jest bardziej dostosowany do zapotrzebowania w laboratorium fizycznym.

Root również posiada własną platformę graficzną (Root 3D Graphics \cite{root3dGraphics}) umożliwiającą tworzenie obrazów na podstawie przeanalizowanych danych. W tej oto platformie początkowo stworzono model detektora ALICE. Bezpośrednie pobranie danych o modelu wiązałoby się z dużym nakładem prac w kierunku przestudiowania struktur danych Root 3D Graphics i stworzenia odpowiedniego programu do przekształcenia danych na konkretny format pliku 3D (na przykład .obj czy .dae). 

Potrzebne rozwiązanie już zostało opracowane w CERN, ale nie zostało jeszcze upublicznione, dlatego na potrzeby niniejszej pracy inżynierskiej otrzymano gotowy plik Collada z geometrią detektora ALICE. Ze względu na to, że niezbędne oprogramowanie jest w trakcie tworzenia, otrzymany plik zawierał błędy w strukturze formatu. Ujawniało się to podczas ładowania modelu z pliku do obiektu aiScene. 

Przy uruchomieniu programu pojawiał się komunikat błędu: \textquotedblleft ERROR::ASSIMP:: Collada: aliceGeom.dae - Expected end of <accessor> element \textquotedblright . Ta wiadomość jest generowana przez funkcję readAccessor w pliku ColladaParser.kt (biblioteka Assimp) i świadczy o błędzie składni podczas użycia funkcji COLLADA <accessor> / <param> \cite{collada}.

Rozwiązano ten problem za pomocą narzędzia Blender, do którego najpierw wczytano nie do końca poprawny plik z modelem detektora, a później wyeksportowano ten sam model do innego pliku, również z rozszerzeniem .dae. Po wykonaniu tych prostych czynności udało się wyświetlić geometrię detektora ALICE. 
%obraz detektora
\newpage
\subsection{Tworzenie obrazu stereoskopowego w technice pasywnej}
Pasywne wizualicje stereoskopowe wymagają przygotowanych wcześniej dwóch obrazów z różniącą się perspektywą i umieszczenia tych obrazów obok siebie. Dla wyrenderowania różniących się perspektyw w OpenGL muszą być dopasowane regulacja asymetryczna wraz z widokiem renderowanego obszaru. Obszar widoku jest równoległym rzutem asymetrycznym w kształcie ściętego stożka lub równoważnym przesunięciem obiektywu w fotografii. 

Opisywana technika nie wprowadza pionowej paralaksy i jest powszechnie stosowana w renderowaniu stereoskopowym \cite{openglCookbook}. Do zilustrowania koncepcji dla prawego oka (dla lewego oka są wykonywane analogiczne transformację w lustrzanym odbiciu) wykorzystano rys. \ref{rys10}. Przesunięcie reprezentuje wielkość odchylenia dla regulacji asymetrycznej ściętego stożka.

\begin{figure}[H]
		\centering
 		\includegraphics[width=10cm]{stereoscopicGL.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{Geometria sceny, którą użytkownik widzi z perspektywy prawego oka \cite{openglCookbook}.}
 		\label{rys10}
\end{figure}

Kod źródłowy umieszczony w tabeli \ref{tab5} obrazuje implementację tworzenia rzutowania i wyświetlania macierzy dla stereoskopowej wizualizacji. Kod wykorzystuje odległość między oczyma (zmienna IOD w kodzie), odległość płaszczyzny obrazu (zmienna depthZ) oraz odległość bliskiej płaszczyzny rzutowania (zmienna nearZ).  Wymienione znienne są użyte do określenia wartości przesunięcia. W przedstawionym kodzie źródłowym są zaimplementowane również obliczenie macierzy projekcji i widoku, skorzystano do tego z funkcji biblioteki GLM. 

W tabeli \ref{tab10} zaprezentowano implementację widoku dla każdego oka poprzez odpowiednie ustawienia macierzy projekcji i widoku. Dla każdego oka położenie kamery jest przekładane o połowę odległości między oczyma (jak zostało pokazane na rys. \ref{rys10}). 

Kod źródłowy w tabelach \ref{tab5} i \ref{tab10} był stworzony w oparciu o \cite{openglCookbook}. Ostateczny wynik renderowania składa się z dwóch oddzielnych obrazów po obu stronach ekranu.
\begin{table}[H]
\caption{Kod źródłowy programu. Implementacja przekształceń macierzy stereoskopowych.}
\label{tab5}
\begin{lstlisting}[frame=single]  % Start your code-block

void computeStereoView(float aspect_ratio, float IOD, float depthZ, 
  bool left_eye, glm::vec3 up = glm::vec3(0.0f, 100.0f, 0.0f), 
  glm::vec3 direction_z = glm::vec3(0, 0, 1) ) {
  float left_right_direction = -1.0f;
  if (left_eye)
     left_right_direction = 1.0f;
  double frustumshift = (IOD / 2) * nearZ / depthZ;
  float top = tan(g_initial_fov / 2.0f) * nearZ;
  float right = aspect_ratio * top + frustumshift * left_right_direction;
  float left = -aspect_ratio * top + frustumshift * left_right_direction;
  float bottom = -top;
  g_projection_matrix = glm::frustum(left,right,bottom,top,nearZ,farZ);
  g_view_matrix = glm::lookAt( g_position - direction_z + 
  	glm::vec3(left_right_direction * IOD / 2, 0, 0),
    g_position + glm::vec3(left_right_direction * IOD / 2, 0, 0), up); }
\end{lstlisting}
\end{table}
\begin{table}[H]
\caption{Kod źródłowy programu. Implementacja ustawień macierzy projekcji i widoku.}
\label{tab10}
\begin{lstlisting}[frame=single]  % Start your code-block

void drawStereo (glm::mat4 transform, Shader shader, int trackSize,
     unsigned int VAO[]) {
 glm::mat4 projection = glm::perspective(glm::radians(camera.Zoom),
     (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1f, 100.0f);
 shader.setMat4("projection", projection);
 glm::mat4 view = camera.GetViewMatrix();
 shader.setMat4("view", view);
 glm::mat4 model = glm::mat4(1.0);
 transform = glm::scale(transform, glm::vec3(0.01, 0.01, 0.01));
 transform = glm::translate(model, glm::vec3(0.0f, 0.0f, -depthZ));
 transform = glm::rotate(model, glm::pi<float>() * rotateY, 
     glm::vec3(0.0f, 1.0f, 0.0f));
 transform = glm::rotate(model, glm::pi<float>() * rotateX, 
     glm::vec3(1.0f, 0.0f, 0.0f));
 GLint transformLoc = glGetUniformLocation(shader.ID, "transform");
 glUniformMatrix4fv(transformLoc, 1, GL_FALSE, glm::value_ptr(transform));
 for (int iterator = 0; iterator < trackSize; iterator++) {
  glBindVertexArray(VAO[iterator]);
  glDrawArrays(GL_LINE_STRIP_ADJACENCY, 0, 4); }}
\end{lstlisting}
\end{table}

\newpage
\subsection{Tworzenie aktywnego obrazu stereoskopowego} 
1) Set the geometry for the view from left human eye 
2) Set the left eye rendering buffers 
3) Render the left eye image 
4) Clear Z-buffer (if the same Z-buffer for left and right image is used) 
5) Set the geometry for the view from right human eye 
6) Set the right eye rendering buffers 
7) Render the right eye image 
8) Swap buffers

\newpage
\subsection{Weryfikacja poprawności i jakości programu}
Weryfikacja poprawności i jakości programu może mieć różny charakter i odnosić się do różnych etapów procesu wytwórczego. W działaniach weryfikacyjnych w niniejszej pracy dyplomowej posługiwano się wskazówkami z książki \cite{specyfikacja}.

W trakcie procesu weryfikacji oceniano, a w niektórych przypadkach ulepszano, kolejno wytwarzane części pracy dyplomowej --- opisy analityczne, elementy projektu, fragmenty kodu, testy. 
\subsubsection{Testy jednostkowe}
Zgodnie z zaleceniami \cite{cleanCode}, testy jednostkowe zostały stworzone dla wszystkich funkcji dostępnych publicznie, w tym funkcji nie zadeklarowanych jako statyczne oraz publicznych konstruktorów i operatorów. Testy obejmują wszystkie główne ścieżki w ramach funkcji i alternatywne gałęzi warunków. Podczas pisania testów jednostkowych trzymano się założenia, że powinny one sprawdzać zarówno przypadki podstawowe, jak i graniczne, dostarczając błędne lub losowe dane, żeby obsługa błędów również została przetestowana.

Główne zasady podczas tworzenia testów jednostkowych:
\begin{itemize}
\itemi jeden test jednostkowy sprawdza tylko jedną rzecz;
\itemi przypadek testowy jest krótki;
\itemi testy przebiegają szybko;
\itemi każdy test działa niezależne, żeby błąd w jednym nie spowodował błędy w innych testach.
\end{itemize}

Głównym celem testów jednostkowych w niniejszej pracy dyplomowej jest sprawdzenie poprawności przekazywania odczytanych danych. Przykładem może posłużyć sprawdzenie czy liczba torów cząstek odczytana z pliku JSON zgadza się z liczbą torów zapisaną do tablicy wierszchołków.

\subsubsection{Zarządzanie jakością projektu}
Po zakończeniu procesu wytwórczego oceniono końcowy produkt --- program wraz z jakością stworzonych obrazów. Zwrócono uwagę również na stopień spełnienia przez projekt wymagań funkcjonalnych i niefunkcjonalnych.

\begin{table}[H]
\caption{Stopień spełnienia wymagań fukcjonalnych.}
\centering
\footnotesize
\label{tab15}
\begin{tabular}{!{\color{sapphire}\vrule width 1pt}m{0.05\textwidth}!{\color{black}\vrule width 1pt}m{0.26\textwidth}!{\color{black}\vrule width 1pt}m{0.57\textwidth}!{\color{sapphire}\vrule width 1pt}}
	\arrayrulecolor{sapphire}\hline
	\Centering\bfseries Nr &
	\Centering\bfseries Nazwa &
	\Centering\bfseries W jakim stopniu jest spełnione \\
	\hline
	\arrayrulecolor{black}
	1 & Renderowanie torów cząstek & Spełnione w 100\%, obrazy umieszczono w Dodatku \ref{Tory}.\\ 
	\hline
	2 & Renderowanie detektora & Spełnione w 100\%.\\ 
	\hline
	3 & Stworzenie efektu widzenia stereoskopowego & Spełnione w 100\%. Przetestowano w laboratorium graficznym z użyciem zestawu 3D.\\ 
	\hline
	4 & Stworzenie efektu aktywnego widzenia stereoskopowego &  \\ 
	\hline
	5 & Możliwość interakcji z użytkownikiem & Spełnione w 100\%, instrukcję użytkownika umieszczono w Dodatku \ref{instrukcja}. \\ 
	\arrayrulecolor{sapphire}\hline
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Stopień spełnienia wymagań niefukcjonalnych.}
\centering
\footnotesize
\label{tab3}
\begin{tabular}{!{\color{sapphire}\vrule width 1pt}m{0.05\textwidth}!{\color{black}\vrule width 1pt}m{0.26\textwidth}!{\color{black}\vrule width 1pt}m{0.57\textwidth}!{\color{sapphire}\vrule width 1pt}}
	\arrayrulecolor{sapphire}\hline
	\Centering\bfseries Nr &
	\Centering\bfseries Nazwa &
	\Centering\bfseries W jakim stopniu jest spełnione \\
	\hline
	\arrayrulecolor{black}
	1 & Przenośność & Spełnione w 100\%. Program bezproblemowo uruchamiano na różnych systemach operacyjnych.\\ 
	\hline
	2 & Zrozumiałość & Spełnione w 100\%, instrukcję użytkownika umieszczono w Dodatku \ref{instrukcja}. \\ 
	\hline
	3 & Czytelność kodu źródłowego & Spełnione w 70\%, niektóre rozwiązania niezgodne z zasadami czystego kodu zastosowano ze względu na specyfikacje bibliotek zewnętrznych. \\ 
	\hline
	4 & Tolerowanie defektów & Spełnione w 100\%, błędne dane mogą być wprowadzone tylko poprzez pliki zewnętrzne. O problemach z plikiem użytkownik zostanie poinformawany za pomocą komunikatów błędu, odpowiednich dla analizatorów składni wybranego formatu. \\
	\hline
	5 & Przyjazność programu dla użytkownika & Spełnione w 100\%, instrukcję użytkownika umieszczono w Dodatku \ref{instrukcja}.\\  
	\arrayrulecolor{sapphire}\hline
\end{tabular}
\end{table}

\newpage
\section{Podsumowanie}
Opracowanie wizualizacji stereoskopowych detektora ALICE wymagało podzielenia całości prac na kilka dużych etapów.

Określenie celów całego przedwsięwzięcia na samym początku wyznaczyło kierunek rozwoju opracowania, wyeliminowało ściezki alternatywne narzucając z góry OpenGL jako podstawową bibliotekę graficzną. Etap pozwolił naszkicować ogólny plan postępowania w dążeniu za wizją niniejszej pracy dyplomowej --- zwiększeniu atrakcyjności fizyki dużych energii poprzez obrazy stereoskopowe. 

Znaczącą cześcią tego opracowania jest etap analizy. Zaczynając od przestudiowania dostępnych technik stereoskopowych aż po poznanie eksperymentów z cząstkami w międzynarodowym laboratorium fizyki cząstek CERN. Już na tym etapie powstał pewien problem nie do ominięcia. Jak to było opisywane wcześniej, pobranie danych z repozytoriów w laboratorium ALICE sprowadziłoby się do stworzenia skomplikowanego parsera plików w języku Root. Dlatego, mimo że nie było to bezpośrednim celem niniejszej pracy, przeanalizowano różne formaty plików 3D oraz biblioteki, które by były w stanie wczytać modele z tych plików do sceny w programie. Dzięki takiej analizie stało się możliwe wykonanie zapytania o model w najbardziej pasującym rozszerzeniu (.DAE). Po części analitycznej zaczęto projektowanie oprogramowania.

Proces projektowania w całości przebiegał zgodnie z zaleceniami z \cite{specyfikacja}. Po ogólnym opisie projektu wyspecyfikowano wymagania funkcjonalne i niefunkcjonalne, określono przypadki użycia


Lukę między opisem wymagań a implementacją wypełnia projekt oprogramowania, definiujący jego elementy składowe, ich zachowanie i reguły współpracy. W ten sposób projekt oprogramowania wyznacza
schemat, zgodnie z którym zostaną napisane i uruchomione wszystkie programy. Rolą
projektanta (architekta, dewelopera) jest stworzenie takiej architektury oprogramowa-
nia, która zapewni spełnienie wymagań, oraz udokumentowanie tej architektury
w postaci odpowiedniego modelu.


, zaplanowano i opisano diagram klas. Na tym etapie również określono biblioteki do użycia w programie. Opracowany projekt był dużym wsparciem podczas implementacji rozwiązania, gdzyż pozwalał kontrolować przebieg prac oraz utrzymywać jakość produktu końcowego na odpowiednim poziomie.

Kolejnym ważnym etapem była implementacja zaporojektowanego programu. W tej części opracowania prowadzono najwiejcej eksperymentów z różnymi wartościami mającymi wpływ na jakość tworzonych obrazów. Ponadto dużą składową etapu implementacji było testowanie. Celem testowania było doświadczalne porównanie programu z projektem. Integralną częścią weryfikacji poprawności programu było wyświetlenie obrazów końcowych z użyciem różnego sprzętu komputerowego i stereoskopowego. Testowanie oprogramowania zakończono pomyślnie, oznacza to, że produkt końcowy spełnia wymagania projektowe i dodatkowo jest akrakcyjny pod względem technologii, z których korzysta.

Tak jak przy przygotowaniu przeważnej większości projektów informatycznych, tak i w niniejszym pojawiały się rozmaite trudności, których nie przewidziano wcześniej. Wbrew tym niedogodnociom zrealizowano większość celów. Reasumując założenia wstępne to:
\begin{itemize}
\itemi analiza dostępnych narzędzi graficznych;
\itemi stworzenie oprogramowania z wykorzystaniem biblioteki OpenGL, które:
	\begin{itemize}
	\itemii wyświetla detektor ALICE;
	\itemii zobrazowuje zderzenia cząstek w eksperymencie ALICE;
	\end{itemize}
\itemi osiągnięcie efektu głębi w stworzonych obrazach za pomocą dwóch technologii;
\itemi zapewnienie przenośności oprogramowania (działa zarówno pod systemem operacyjnym Linux jak i Windows).
\end{itemize}

Nie do końca został zrealizowany cel przenośności oprogramowania. Jest to związane z wymaganiami systemowymi obsługi stereoskopii aktywnej. Obecnie liderem na rynku aktywnych obrazów stereoskopowych jest technologia Nvidia 3D Vision, która wymaga operacyjnego systemu Windows dla poprawnego działania sprzętu i sterowników. Niestety nie zostało opublikowane jeszcze rozwiązanie dla innych systemów operacyjnych. Pozostałe cele zostały w pełni zrealizowane. 

\subsection{Ciąg dalszy prac}
Wizualizacje 3D w nauce są bardzo obiecującym obszarem zastosowania. W kolejnym opracowaniu temat streoskopii zostałby rozszerzony do wirtualnej rzeczywistości. Wtedy zestaw VR (ang. Virtual Reality, wirtualna rzeczywistość) oferowałby szansę zobaczenia akceleratora LHC, detektora i centrum danych z bliska.

Obecnie potężne aplikacje do wizualizacji naukowych mogą być rozwijane przy użyciu istniejących technologii dość małym kosztem. Tendencja do projektowania zwykłych komputerów w kierunku szybszej grafiki, większej mocy obliczeniowej, systemów operacyjnych wspierających współbiezność i bardzo dużej pamięci powinna wesprzeć i przyspieszyć rozwój rzeczywistości wirtualnej.

Wykorzystanie najnowszych technologii skróciłoby dystans między złożoną terminologią naukową a szerokim zrozumieniem społecznym. Poprzez zanurzenie publiczności do w pełni interaktywnego środowiska możnaby zwizualizować trudne pojęcia fizyczne. Wypełniłoby to lukę między nauką, interaktywnymi mediami i wizualizacją informacji, tworząc zabawne narzędzie do wypróbowania.

\subsection{Zakończenie}
Wizualizacje trójwymiarowe w celach dydaktycznych czy naukowych stanowią nieoceniony napęd w rozwoju komputerów i nowych technologii.   Tak duży postęp narzędzi multimedialnych w ostatnich czasach sprawia, że coraz częściej wizualizacje trójwymiarowe można powszechnie spotkać na uczelniach lub w centrach rozrywki.

Oczywiście obrazy trójwymiarowe przedstawiane na zwykłym ekranie 2D bez zastosowania sprzętu specjalistycznego (okularów 3D, emitera podczerwieni w przypadku stereoskopii aktywnej) są prymitywną symulacją, przedstawieniem sceny trójwymiarowej na płaszczyźnie o dwóch wymiarach. Jednak poprzez połączenie technologii wirtualnej rzeczywistości z animacją można uzyskać kolosalne narzędzie edukacyjne, które by było potężnym wsparciem dla stosowanych w tej chwili metodyk. 

Mogły by one obrazować pojedyńcze zjawiska czy procesy w warunkach mocno zbliżonych do rzeczywistego środowiska.
\newpage
\begin{appendices}
\section{Wyniki tworzenia obrazów torów cząstek}
\label{Tory}
\begin{figure}[H]
	\begin{subfigure}{0.50\textwidth}
		\centering
 		\includegraphics[width=\textwidth]{trackScreen1.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{}
 		\label{rys18}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.50\textwidth}
		\centering
		\includegraphics[width=\textwidth]{trackScreen3.png}
		\captionsetup{font={up, footnotesize}}
    	\caption{}
		\label{rys19}
	\end{subfigure}
	
	\vskip\baselineskip
		\begin{subfigure}{0.50\textwidth}
		\centering
 		\includegraphics[width=\textwidth]{trackScreen4.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{}
 		\label{rys20}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.50\textwidth}
		\centering
		\includegraphics[width=\textwidth]{trackScreen5.png}
		\captionsetup{font={up, footnotesize}}
    	\caption{}
		\label{rys21}
	\end{subfigure}
	
	\vskip\baselineskip
		\begin{subfigure}{0.50\textwidth}
		\centering
 		\includegraphics[width=\textwidth]{trackScreen8.png}
 		\captionsetup{font={up, footnotesize}}
    	\caption{}
 		\label{rys22}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.50\textwidth}
		\centering
		\includegraphics[width=\textwidth]{trackScreen9.png}
		\captionsetup{font={up, footnotesize}}
    	\caption{}
		\label{rys23}
	\end{subfigure}
    \label{rys}
\end{figure}

\newpage
\section{Wyniki tworzenia obrazów w technice stereoskopii pasywnej}
\label{sideBside}

\newpage
\section{Instrukcja użytkowania programu}
\label{instrukcja}
Sterowanie widokiem kamery w programie wygląda następująco:
\begin{itemize}
\itemi $\uparrow$, $\downarrow$, $\leftarrow$, $\rightarrow$ --- sterowanie położeniem obserwatora;
\itemi ruchy w prawo lub lewo myszą --- rozglądanie się;
\itemi kółko myszy --- przyciąganie lub oddalanie obrazu;
\itemi klawisz t --- wyświetlanie torów cząstek;
\itemi klawisz d --- wyświetlenie detektora;
\itemi klawisz s --- widok stereoskopii pasywnej;
\itemi klawisz u --- widok zwykły;
\end{itemize}
\end{appendices}

\newpage
\bibliographystyle{plain} % bibliography style
\bibliography{bibliography} % add bibliography
\newpage
\listoffigures
\newpage
\listoftables

\end{document}
